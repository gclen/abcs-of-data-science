<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>K is for K-fold cross-validation | ABCs of data science</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="K is for K-fold cross-validation" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In supervised learning problems we have data as well as a label (something we need to predict) for each data point. We typically refer to the combination of data + labels as training data. You might remember from F is for F1 score that there are two broad categories of supervised learning problems: classification and regression. In classification, we are trying to predict which class (or category) the input data belongs to. For example, this might be trying to predict if an email is spam or not spam. There are typically a small number of categories to choose from (e.g. spam/not spam). In regression problems, we are trying to predict a continuous variable (i.e. a number) such as the selling price of a house based on some features (e.g. square footage). Here we are not trying to pick from a small group of categories, but to get close to the actual number (where there can be infinitely possible values)." />
<meta property="og:description" content="In supervised learning problems we have data as well as a label (something we need to predict) for each data point. We typically refer to the combination of data + labels as training data. You might remember from F is for F1 score that there are two broad categories of supervised learning problems: classification and regression. In classification, we are trying to predict which class (or category) the input data belongs to. For example, this might be trying to predict if an email is spam or not spam. There are typically a small number of categories to choose from (e.g. spam/not spam). In regression problems, we are trying to predict a continuous variable (i.e. a number) such as the selling price of a house based on some features (e.g. square footage). Here we are not trying to pick from a small group of categories, but to get close to the actual number (where there can be infinitely possible values)." />
<link rel="canonical" href="https://abcsofdatascience.ca/blog/k-is-for-kfold-cross-validation" />
<meta property="og:url" content="https://abcsofdatascience.ca/blog/k-is-for-kfold-cross-validation" />
<meta property="og:site_name" content="ABCs of data science" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-10-05T00:00:00-05:00" />
<script type="application/ld+json">
{"datePublished":"2020-10-05T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://abcsofdatascience.ca/blog/k-is-for-kfold-cross-validation"},"description":"In supervised learning problems we have data as well as a label (something we need to predict) for each data point. We typically refer to the combination of data + labels as training data. You might remember from F is for F1 score that there are two broad categories of supervised learning problems: classification and regression. In classification, we are trying to predict which class (or category) the input data belongs to. For example, this might be trying to predict if an email is spam or not spam. There are typically a small number of categories to choose from (e.g. spam/not spam). In regression problems, we are trying to predict a continuous variable (i.e. a number) such as the selling price of a house based on some features (e.g. square footage). Here we are not trying to pick from a small group of categories, but to get close to the actual number (where there can be infinitely possible values).","@type":"BlogPosting","url":"https://abcsofdatascience.ca/blog/k-is-for-kfold-cross-validation","headline":"K is for K-fold cross-validation","dateModified":"2020-10-05T00:00:00-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://abcsofdatascience.ca/feed.xml" title="ABCs of data science" /><link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>K is for K-fold cross-validation | ABCs of data science</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="K is for K-fold cross-validation" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In supervised learning problems we have data as well as a label (something we need to predict) for each data point. We typically refer to the combination of data + labels as training data. You might remember from F is for F1 score that there are two broad categories of supervised learning problems: classification and regression. In classification, we are trying to predict which class (or category) the input data belongs to. For example, this might be trying to predict if an email is spam or not spam. There are typically a small number of categories to choose from (e.g. spam/not spam). In regression problems, we are trying to predict a continuous variable (i.e. a number) such as the selling price of a house based on some features (e.g. square footage). Here we are not trying to pick from a small group of categories, but to get close to the actual number (where there can be infinitely possible values)." />
<meta property="og:description" content="In supervised learning problems we have data as well as a label (something we need to predict) for each data point. We typically refer to the combination of data + labels as training data. You might remember from F is for F1 score that there are two broad categories of supervised learning problems: classification and regression. In classification, we are trying to predict which class (or category) the input data belongs to. For example, this might be trying to predict if an email is spam or not spam. There are typically a small number of categories to choose from (e.g. spam/not spam). In regression problems, we are trying to predict a continuous variable (i.e. a number) such as the selling price of a house based on some features (e.g. square footage). Here we are not trying to pick from a small group of categories, but to get close to the actual number (where there can be infinitely possible values)." />
<link rel="canonical" href="https://abcsofdatascience.ca/blog/k-is-for-kfold-cross-validation" />
<meta property="og:url" content="https://abcsofdatascience.ca/blog/k-is-for-kfold-cross-validation" />
<meta property="og:site_name" content="ABCs of data science" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-10-05T00:00:00-05:00" />
<script type="application/ld+json">
{"datePublished":"2020-10-05T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://abcsofdatascience.ca/blog/k-is-for-kfold-cross-validation"},"description":"In supervised learning problems we have data as well as a label (something we need to predict) for each data point. We typically refer to the combination of data + labels as training data. You might remember from F is for F1 score that there are two broad categories of supervised learning problems: classification and regression. In classification, we are trying to predict which class (or category) the input data belongs to. For example, this might be trying to predict if an email is spam or not spam. There are typically a small number of categories to choose from (e.g. spam/not spam). In regression problems, we are trying to predict a continuous variable (i.e. a number) such as the selling price of a house based on some features (e.g. square footage). Here we are not trying to pick from a small group of categories, but to get close to the actual number (where there can be infinitely possible values).","@type":"BlogPosting","url":"https://abcsofdatascience.ca/blog/k-is-for-kfold-cross-validation","headline":"K is for K-fold cross-validation","dateModified":"2020-10-05T00:00:00-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://abcsofdatascience.ca/feed.xml" title="ABCs of data science" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">ABCs of data science</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">K is for K-fold cross-validation</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-10-05T00:00:00-05:00" itemprop="datePublished">
        Oct 5, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      7 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#data_science">data_science</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#supervised_learning">supervised_learning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#AI">AI</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>In supervised learning problems we have data as well as a label (something we need to predict) for each data point. We typically refer to the combination of data + labels as <strong>training data</strong>. You might remember from <a href="(/blog/f-is-for-f1)">F is for F1 score</a> that there are two broad categories of supervised learning problems: classification and regression. In classification, we are trying to predict which class (or category) the input data belongs to. For example, this might be trying to predict if an email is spam or not spam. There are typically a small number of categories to choose from (e.g. spam/not spam). In regression problems, we are trying to predict a continuous variable (i.e. a number) such as the selling price of a house based on some features (e.g. square footage). Here we are not trying to pick from a small group of categories, but to get close to the actual number (where there can be infinitely possible values).</p>

<p>In all supervised learning problems we want to know well our model is doing and how well it can predict things for unseen data.</p>

<h3 id="how-does-our-model-fit-the-data">How does our model fit the data?</h3>

<p>Lets focus on regression and take a further look at the housing example. For now assume we have a training set and some way to model our data. How do we know if our model is any good? Let’s take a look at our imaginary training data</p>

<p><img src="/images/k_is_for_kfold_cv/house_price_dataset.png" alt="" /></p>

<p>As you can imagine, as the square footage of a house increases the price goes up. If our model tries to fit this data with a straight line then we get something like this</p>

<p><img src="/images/k_is_for_kfold_cv/house_price_underfit.png" alt="" /></p>

<p>This line is an okay fit to the data but doesn’t really give accurate predictions. We say that this model <strong>underfits</strong> the data or has <strong>high bias</strong>. What we really want is a function that is much closer to all the points</p>

<p><img src="/images/k_is_for_kfold_cv/house_price_proper_fit.png" alt="" /></p>

<p>This is a pretty good fit to the data and seems to do what one would intuitively expect. The function doesn’t pass through every point but it’s pretty close to most of them. Our model could probably find some function that passes through every point</p>

<p><img src="/images/k_is_for_kfold_cv/house_price_overfit.png" alt="" /></p>

<p>While this function passes through every point it won’t work well on new data. We say that this model <strong>overfits</strong> the data or has <strong>high variance</strong>. This means that the model has two many parameters or there are not enough features in the training set. In this case it would probably be beneficial to add more features to the training set. For example, there is a huge difference in price between a 1000 square foot house in San Francisco versus Thunder Bay. So adding the location of the house might improve the model. In the case of the model that underfit the data, adding more features won’t help. We need to add more parameters to the model so that there is more room to try and fit the data.</p>

<h3 id="testing-our-model">Testing our model</h3>

<p>So how do we tell if our model is any good? Typically, the training data is broken into 2 parts. There is a <strong>training set</strong> (about 70% of the data) and a <strong>test set</strong> (the remaining 30%). Splitting the data into training/test sets is known as <strong>cross-validation</strong>. As the name implies the parameters of the model are trained using the training set. The model is then fed in the data from the test set and the <a href="/blog/f-is-for-f1">error</a> is calculated. Using the error associated with both the training and the test sets you can diagnose if your model underfits, overfits, or is a good fit to the data.</p>

<p>In addition to using a train/test split, people will often use a third split: the <strong>validation set</strong> (sometimes called a development set). Let’s break down what each of those sets are used for:</p>

<ul>
  <li>Training set: As you would expect, this is the data used to actually train the model. As you might <a href="/blog/g-is-for-gradient-descent">recall</a>, training a model just means learning which feature weights give you the best predictions (i.e. match your labels as closely as possible).</li>
  <li>Validation set: This is used for learning the best set of <strong>hyperparameters</strong>. Hyperparameters are knobs you can turn for the model itself (e.g. learning rate or mini-batch size). In the training set where we are trying to answer “how can I weight these features to get the best predictions?”. In the validation set we are trying to answer “how can I adjust the model itself to give the best predictions when training?”</li>
  <li>Test set: We want to test our model on data that it has not seen in order to see if it is overfitting/underfitting. We never update the model (or hyperparameters) on the test set and we just make predictions on it using the trained model.</li>
</ul>

<h3 id="k-fold-cross-validation">K-fold cross-validation</h3>

<p>If you split your data into training/validation/test sets using a 60/20/20 split then that means you can only use 60% of your available data for model training and the validation set is a fixed 20% of the data. In cases where you don’t have a lot of training data (which is often) this extra 20% can make a big difference! K-fold cross-validation lets you get the benefits of having a validation set without having to hold out 20% of your training data.</p>

<p>In k-fold cross-validation you have a training set and what I will call the true test set. This test set is exactly what is described above: unseen data used to test predictions. The k-fold part comes from taking your training set and splitting it into k groups (or folds). The value of k can be any number that you choose but as an example let’s say k is 5. You would split your training set into 5 folds and train the model 5 times. The first time you train, the first fold is used as the “test” set and the remaining 4 folds are used as a training set. The second time you train, you use the second fold as a “test” set and the other 4 folds are used as a training set. You repeat this process until you’ve used each fold as a test set. After you have fully trained the model you test your predictions on the true test set as before. This process is illustrated in the image below (taken from the wonderful scikit-learn documentation). This process can be more computationally expensive since you need to train k models instead of 1 but also means you don’t need a held out validation set.</p>

<p><img src="/images/k_is_for_kfold_cv/sklearn_kfold_cross_validation.png" alt="" title="Taken from https://scikit-learn.org/stable/_images/grid_search_cross_validation.png" /></p>

<p>How do you choose a value for k? There are some different methods for choosing k since a wrong value can lead you to be overconfident in your model:</p>

<ul>
  <li>Pick k=10. This is a decent starting point (or k=5) and has been shown to work in a wide variety of applications.</li>
  <li>Pick a value of k so that each fold gives a statistically representative sample size</li>
  <li>Let k be the number of points you have in your training set so that each point can be used in the hold out test set. This is also referred to as leave-one-out cross-validation.</li>
</ul>

<h3 id="summary">Summary</h3>

<p>Evaluating the performance of supervised learning models is critical and you want to be sure that your models aren’t overfitting/underfitting the training data. Cross-validation is a technique that should be used in all supervised learning applications to check that your model is behaving as expected. It is also important to keep in mind that even if your model generalizes well to the test set, it may not generalize to the real world if your training data is not truly representative.</p>

<h3 id="other-resources">Other resources</h3>

<ul>
  <li><a href="https://scikit-learn.org/stable/modules/cross_validation.html">Cross-validation explanation in scikit-learn</a></li>
  <li><a href="https://machinelearningmastery.com/k-fold-cross-validation/">K-fold cross-validation explanation</a></li>
</ul>


  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="gclen/abcs-of-data-science"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blog/k-is-for-kfold-cross-validation" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>A gentle introduction to many data science concepts for readers of all backgrounds</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/gclen" title="gclen"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/abcsofdatasci" title="abcsofdatasci"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
