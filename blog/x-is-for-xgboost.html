<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>X is for XGBoost | ABCs of data science</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="X is for XGBoost" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="XGBoost is a software package/algorithm that has been used to train impressive models in recent years (particularly on tabular/structured data). It’s also extremely fast and has wrappers in a wide variety of languages (Python, C++, Ruby, R, etc). For these reasons, as well as the fact that it has been used to win many Kaggle competitions it has become very popular in the past few years. XGBoost stands for “Extreme Gradient Boosting”, which despite sounding like something on the side of a pre-workout supplement, is a very useful technique for improving ensembles. In this blog I’ll explain how XGBoost and related methods work and the types of problems that it’s applicable for." />
<meta property="og:description" content="XGBoost is a software package/algorithm that has been used to train impressive models in recent years (particularly on tabular/structured data). It’s also extremely fast and has wrappers in a wide variety of languages (Python, C++, Ruby, R, etc). For these reasons, as well as the fact that it has been used to win many Kaggle competitions it has become very popular in the past few years. XGBoost stands for “Extreme Gradient Boosting”, which despite sounding like something on the side of a pre-workout supplement, is a very useful technique for improving ensembles. In this blog I’ll explain how XGBoost and related methods work and the types of problems that it’s applicable for." />
<link rel="canonical" href="https://abcsofdatascience.ca/blog/x-is-for-xgboost" />
<meta property="og:url" content="https://abcsofdatascience.ca/blog/x-is-for-xgboost" />
<meta property="og:site_name" content="ABCs of data science" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-02-05T00:00:00-06:00" />
<script type="application/ld+json">
{"description":"XGBoost is a software package/algorithm that has been used to train impressive models in recent years (particularly on tabular/structured data). It’s also extremely fast and has wrappers in a wide variety of languages (Python, C++, Ruby, R, etc). For these reasons, as well as the fact that it has been used to win many Kaggle competitions it has become very popular in the past few years. XGBoost stands for “Extreme Gradient Boosting”, which despite sounding like something on the side of a pre-workout supplement, is a very useful technique for improving ensembles. In this blog I’ll explain how XGBoost and related methods work and the types of problems that it’s applicable for.","dateModified":"2021-02-05T00:00:00-06:00","datePublished":"2021-02-05T00:00:00-06:00","headline":"X is for XGBoost","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://abcsofdatascience.ca/blog/x-is-for-xgboost"},"url":"https://abcsofdatascience.ca/blog/x-is-for-xgboost","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://abcsofdatascience.ca/feed.xml" title="ABCs of data science" /><link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>X is for XGBoost | ABCs of data science</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="X is for XGBoost" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="XGBoost is a software package/algorithm that has been used to train impressive models in recent years (particularly on tabular/structured data). It’s also extremely fast and has wrappers in a wide variety of languages (Python, C++, Ruby, R, etc). For these reasons, as well as the fact that it has been used to win many Kaggle competitions it has become very popular in the past few years. XGBoost stands for “Extreme Gradient Boosting”, which despite sounding like something on the side of a pre-workout supplement, is a very useful technique for improving ensembles. In this blog I’ll explain how XGBoost and related methods work and the types of problems that it’s applicable for." />
<meta property="og:description" content="XGBoost is a software package/algorithm that has been used to train impressive models in recent years (particularly on tabular/structured data). It’s also extremely fast and has wrappers in a wide variety of languages (Python, C++, Ruby, R, etc). For these reasons, as well as the fact that it has been used to win many Kaggle competitions it has become very popular in the past few years. XGBoost stands for “Extreme Gradient Boosting”, which despite sounding like something on the side of a pre-workout supplement, is a very useful technique for improving ensembles. In this blog I’ll explain how XGBoost and related methods work and the types of problems that it’s applicable for." />
<link rel="canonical" href="https://abcsofdatascience.ca/blog/x-is-for-xgboost" />
<meta property="og:url" content="https://abcsofdatascience.ca/blog/x-is-for-xgboost" />
<meta property="og:site_name" content="ABCs of data science" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-02-05T00:00:00-06:00" />
<script type="application/ld+json">
{"description":"XGBoost is a software package/algorithm that has been used to train impressive models in recent years (particularly on tabular/structured data). It’s also extremely fast and has wrappers in a wide variety of languages (Python, C++, Ruby, R, etc). For these reasons, as well as the fact that it has been used to win many Kaggle competitions it has become very popular in the past few years. XGBoost stands for “Extreme Gradient Boosting”, which despite sounding like something on the side of a pre-workout supplement, is a very useful technique for improving ensembles. In this blog I’ll explain how XGBoost and related methods work and the types of problems that it’s applicable for.","dateModified":"2021-02-05T00:00:00-06:00","datePublished":"2021-02-05T00:00:00-06:00","headline":"X is for XGBoost","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://abcsofdatascience.ca/blog/x-is-for-xgboost"},"url":"https://abcsofdatascience.ca/blog/x-is-for-xgboost","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://abcsofdatascience.ca/feed.xml" title="ABCs of data science" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">ABCs of data science</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">X is for XGBoost</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-02-05T00:00:00-06:00" itemprop="datePublished">
        Feb 5, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      4 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#data_science">data_science</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#supervised_learning">supervised_learning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#xgboost">xgboost</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#ensembles">ensembles</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p><a href="https://xgboost.readthedocs.io/en/latest/">XGBoost</a> is a software package/algorithm that has been used to train impressive models in recent years (particularly on tabular/structured data). It’s also extremely fast and has wrappers in a wide variety of languages (Python, C++, Ruby, R, etc). For these reasons, as well as the fact that it has been used to win many <a href="https://www.kaggle.com/">Kaggle</a> competitions it has become very popular in the past few years. XGBoost stands for “Extreme Gradient Boosting”, which despite sounding like something on the side of a pre-workout supplement, is a very useful technique for improving ensembles. In this blog I’ll explain how XGBoost and related methods work and the types of problems that it’s applicable for.</p>

<h3 id="ensembling">Ensembling</h3>

<p>If you remember from <a href="/blog/s-is-for-supervised-learning">“S is for Supervised Learning”</a> an ensemble is a collection of weak learners (such as decision trees). The idea is that each learner on its own is not particularly strong, but in aggregate they will become a stronger learner. There are two main methods to combine the predictions of the weak learners: <strong>averaging</strong> and <strong>boosting</strong>.</p>

<h5 id="averaging-methods">Averaging methods</h5>

<p>In a random forest model, you might have 500 decision trees which are all trained on different subsets of the data as well as different features. Each tree gets a vote based on their prediction and the class with the majority of votes is chosen as the prediction (this is analogous to the <a href="https://en.wikipedia.org/wiki/Wisdom_of_the_crowd">wisdom of the crowds</a>). Let’s imagine you are trying to predict if something is a <a href="https://www.theincomparable.com/robot/">Robot or Not</a>. If 70% of the trees predict “robot”, and 30% of the trees predict “not a robot” then the overall prediction will be “robot”.  This an example of an <strong>averaging ensemble</strong> because we trained 500 trees independently and then averaged the predictions.</p>

<h5 id="boosting-methods">Boosting methods</h5>

<p>In <strong>boosting</strong> we train the individual learners sequentially instead of independently. In theory, the 2nd decision tree can learn from the mistakes of the first, the 3rd tree from the 2nd and so on.  At the very least we hope it will make <em>different</em> mistakes. <a href="https://scikit-learn.org/stable/modules/ensemble.html#adaboost">AdaBoost</a> (Adaptive Boosting) is a popular method for boosting and is relatively straightforward to understand.</p>

<p>Let’s imagine we are trying to predict which players should be drafted by the NBA from draft eligible players. We have three features: the players height, average number of points scored per game in their last two seasons, and their teams winning percentage. AdaBoost typically uses <strong>decision stumps</strong> (as compared to trees) as its weak learner. A decision stump is just a very shallow tree (with only one node and two leaves). This means each stump only looks at one variable (e.g. height) at a time. The decision stump could learn that if a player is taller than 6’6” then they should be drafted (and not drafted if they are shorter than that). Here are the steps of how AdaBoost works:</p>

<ol>
  <li>Train a weak classifier (e.g. our decision stump using height as a variable) on all of the data. Each data point is given a weight (initially they are all equal).</li>
  <li>Create a decision stump for all variables (height, PPG, average wins). After training check how well each stump performs (did it classify the points correctly?).</li>
  <li>For all of the data points which were not classified correctly, increase their weight. For correctly classified samples, decrease their weight. This forces the overall model to focus on points that are hard to classify.</li>
  <li>Repeat steps 2 and 3 until you reach the maximum number of iterations (or all points have been correctly classified).</li>
</ol>

<p>At the end, all of the weak learners get a weighted vote to produce the final prediction.</p>

<h5 id="gradient-boosted-trees">Gradient boosted trees</h5>

<p>If we want to train an ensemble of decision trees how should we do it? We can generalize boosting on decision trees to use arbitrary <a href="/blog/s-is-for-supervised-learning">loss functions</a>. Once we’ve picked a loss function we can use <a href="/blog/g-is-for-gradient-descent">gradient descent</a> to optimize it. Using this technique is typically called <strong>Gradient Boosted Decision Trees</strong> or <strong>Gradient Boosted Trees</strong>.</p>

<h3 id="when-should-i-use-ensembles-of-decision-trees">When should I use ensembles of decision trees?</h3>

<p>Models like Random Forests or Gradient Boosted Trees are a good starting point for many problems.</p>

<ul>
  <li>They can be used for both classification and regression</li>
  <li>They work on a wide variety of datasets. In particular, they work well on tabular data (think Excel spreadsheets)</li>
  <li>Random forests handle missing values and categorical data well</li>
  <li>They typically handle high dimensional data (large numbers of features) well</li>
  <li>You can typically <a href="/blog/i-is-for-interpretability">interpret</a> the predictions made by these models</li>
</ul>

<h3 id="summary">Summary</h3>

<p>XGBoost is a library which implements gradient boosting and is extremely fast. It’s available in a wide variety of languages and frameworks. For Python you can <a href="https://xgboost.readthedocs.io/en/latest/build.html">install it</a> using</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install xgboost
</code></pre></div></div>

<p>Even if you don’t use XGBoost, I recommend starting with decision tree based models as a starting point for most ML problems. You can often get further with some other models but they typically require more hyperparameter optimization and deep understanding of your data.</p>

<h3 id="other-resources">Other resources</h3>

<ul>
  <li><a href="http://lineardigressions.com/episodes/2017/1/22/ensemble-algorithms">Episode of Linear Digressions on Ensembles</a></li>
  <li><a href="https://scikit-learn.org/stable/modules/ensemble.html">Scikit-learn documentation on ensemble methods</a></li>
  <li><a href="https://xgboost.readthedocs.io/en/latest/index.html">XGBoost documentation</a></li>
  <li><a href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html">Introduction to boosted trees from XGBoosts documentation</a></li>
  <li><a href="https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/">A Gentle Introduction to XGBoost for Applied Machine Learning</a></li>
</ul>


  </div>

  <div class="PageNavigation">
    
        <a class="prev" href="/blog/w-is-for-wasserstein-gans">&laquo; W is for Wasserstein GANs</a>
    
    
        <a class="next" href="/blog/y-is-for-you-should-talk-to-your-clients">Y is for You Should Talk to Your Clients &raquo;</a>
    
  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="gclen/abcs-of-data-science"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blog/x-is-for-xgboost" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>A gentle introduction to many data science concepts for readers of all backgrounds</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/gclen" target="_blank" title="gclen"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/abcsofdatasci" target="_blank" title="abcsofdatasci"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
