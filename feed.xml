<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://abcsofdatascience.ca/feed.xml" rel="self" type="application/atom+xml" /><link href="https://abcsofdatascience.ca/" rel="alternate" type="text/html" /><updated>2020-10-23T19:15:07-05:00</updated><id>https://abcsofdatascience.ca/feed.xml</id><title type="html">ABCs of data science</title><subtitle>A gentle introduction to many data science concepts for readers of all backgrounds</subtitle><entry><title type="html">O is for Outlier Detection</title><link href="https://abcsofdatascience.ca/blog/o-is-for-outlier-detection" rel="alternate" type="text/html" title="O is for Outlier Detection" /><published>2020-10-23T00:00:00-05:00</published><updated>2020-10-23T00:00:00-05:00</updated><id>https://abcsofdatascience.ca/blog/o-is-for-outlier-detection</id><content type="html" xml:base="https://abcsofdatascience.ca/blog/o-is-for-outlier-detection">&lt;p&gt;Outlier (or anomaly) detection is the technical term for “finding weird stuff”. It’s used in a wide variety of applications including malware detection and looking for credit card fraud. For example, if you live in Ottawa but your credit card was used to buy a gaming console in Boise, Idaho (without any other purchases) that would be anomalous. Outlier detection is related to &lt;a href=&quot;/blog/c-is-for-clustering&quot;&gt;clustering&lt;/a&gt;. In clustering we are trying to find the groups of related data. In outlier detection we are trying to find the points that don’t belong to any groups. There are three different categories of outliers&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Local (or contextual) outliers&lt;/strong&gt;: These are points that are close to groups of data but don’t belong to any cluster. This could be an email which seems mostly legitimate except something seems a little off (e.g. “Follow us on Twittterr”) .&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Global (or point) outliers&lt;/strong&gt;: These are the data points that are completely off on their own and are far away from other data points. Going back to the email example, this would be like having an email in German if the rest of your inbox had English/French emails.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Collective outliers&lt;/strong&gt; : These are groups of outlying points which may have some underlying pattern. This would be like having a spam campaign of emails where they look strange compared to normal email, but are all related to one another.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/o_is_for_outlier_detection/outlier_data.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In many (all?) large datasets there are bound to be lots of outliers. It’s like being a woman trying to find a partner on a dating app: “the odds are good but the goods are odd”. It’s relatively easy to find anomalies but it’s more challenging to find &lt;strong&gt;interesting&lt;/strong&gt; outliers.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It’s worth noting that in a lot of cases if something is anomalous it doesn’t necessarily mean that it is bad. It just means that the data point is different from the others. However, it probably means that it is more interesting and may need to be investigated.&lt;/li&gt;
  &lt;li&gt;To find out if an outlier is truly interesting or not you need to combine it with extra context. For example, if you find an anomalous credit card purchase (such as a gaming console in Idaho) it’s worth looking at the other purchases around that time (did they also buy plane tickets to Idaho?).&lt;/li&gt;
  &lt;li&gt;You can also find interesting patterns by grouping anomalies together. In malware detection this could mean “did a bunch of suspicious activity happen on one computer in a short time frame?”. Here you need something else to group on (in this case it’s by looking at anomalies for one computer at a time).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;how-do-we-actually-find-anomalies&quot;&gt;How do we actually find anomalies?&lt;/h3&gt;

&lt;h5 id=&quot;low-frequency-events&quot;&gt;Low frequency events&lt;/h5&gt;

&lt;p&gt;One of the easiest ways to find outliers is to say “show me events that occur less than X% of the time”. This is really easy because we can just count all the events we are interested in and then divide by the total number of events. Let’s imagine we have a survey of what people put in their coffee&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;Raw count&lt;/th&gt;
      &lt;th&gt;Frequency (count/total)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Nothing (black)&lt;/td&gt;
      &lt;td&gt;95&lt;/td&gt;
      &lt;td&gt;0.35&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Milk&lt;/td&gt;
      &lt;td&gt;120&lt;/td&gt;
      &lt;td&gt;0.44&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Sugar&lt;/td&gt;
      &lt;td&gt;53&lt;/td&gt;
      &lt;td&gt;0.20&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Butter&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0.01&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;In this case it’s pretty clear that people &lt;a href=&quot;https://www.nytimes.com/2014/12/14/style/the-cult-of-the-bulletproof-coffee-diet.html&quot;&gt;putting butter in their coffee&lt;/a&gt; is weird. This method breaks down if there are lots of different options or one or two very popular options.&lt;/p&gt;

&lt;h5 id=&quot;modelling-the-distribution-of-your-data&quot;&gt;Modelling the distribution of your data&lt;/h5&gt;

&lt;p&gt;One way to find outliers is to look at how far a point deviates from the average (mean). If your data is &lt;strong&gt;normally distributed&lt;/strong&gt; (i.e. looks like a bell curve) you can look at how many standard deviations a point is from the mean. In a normal distribution 68% of the data falls within one standard deviation, 95% within two standard deviations etc. For example, if the average height of a man is 5’10” and the standard deviation is 4” then 68% of men are between 5’6” and 6’2”. If you have a man who is 7 feet tall, then they would be more than 3 standard deviations away from the mean. It’s fair to say that this person is abnormally tall (unless you compare them only to &lt;a href=&quot;https://www.youtube.com/watch?v=88w_cA1QyeQ&quot;&gt;NBA players&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/o_is_for_outlier_detection/normal_distribution.png&quot; alt=&quot;&quot; title=&quot;Taken from https://upload.wikimedia.org/wikipedia/commons/a/a9/Empirical_Rule.PNG&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;using-clustering-to-find-outliers&quot;&gt;Using clustering to find outliers&lt;/h5&gt;

&lt;p&gt;Some clustering algorithms (such as &lt;a href=&quot;/blog/h-is-for-hdbscan&quot;&gt;HDBSCAN&lt;/a&gt;) label points which don’t fall into any cluster as outliers. These points could be global or local outliers. They may also include the probability that a point belongs in a cluster. This can be used to find local outliers by looking for points with relatively low probabilities. If we look back to the sample dataset in my post on HDBSCAN, all of the grey points are listed as noise/outliers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/o_is_for_outlier_detection/sample_data_clustered.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;time-series-modelling&quot;&gt;Time series modelling&lt;/h5&gt;

&lt;p&gt;In many cases what counts as an anomaly depends on when it occurs. For example, a store doing $100 000 in sales in one day could be abnormally high in February but unusually low in the holiday season. The average amount of sales changes throughout the year so we need to take time into account when doing outlier detection. &lt;a href=&quot;https://facebook.github.io/prophet/&quot;&gt;Prophet&lt;/a&gt; is a popular python library for dealing with time series data.&lt;/p&gt;

&lt;h5 id=&quot;isolation-forests&quot;&gt;Isolation forests&lt;/h5&gt;

&lt;p&gt;One way we can find anomalies is using a method called &lt;strong&gt;isolation forests&lt;/strong&gt;.  Isolation forests take advantage of two key properties of anomalies:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;There are fewer of them&lt;/li&gt;
  &lt;li&gt;They have attributes that are different than most of the points&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These properties mean that the anomalies are more prone to isolation. What do I mean by isolation? Imagine I have the data points shown in blue below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/o_is_for_outlier_detection/isolated_forest_points_no_cuts.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The idea behind isolated forests is to keep making cuts in the data so that each point is in its own partition. From these cuts we can build up trees to use in our forest. For example:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/o_is_for_outlier_detection/isolated_forest_points.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this case, the first cut is shown in red. Everything above it is in one partition (which in this case is an isolated point), while everything below it is in another. We can make a cut to the bottom partition (shown in black) and isolate another point. We can keep cutting and build up the tree. At the end, every leaf node will correspond to an isolated point. The hypothesis is that anomalies will be closer to the root node than “normal” points. The algorithm works as follows:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Take a random subset of the data and build up the tree for that subset.&lt;/li&gt;
  &lt;li&gt;Repeat step 1 a bunch of times. Since all of the cuts are fairly arbitrary, we will use an ensemble of trees to average out our choice of cuts.&lt;/li&gt;
  &lt;li&gt;From the forest of trees calculate the average path length to each leaf node. This just means “on average how many cuts did we need to make to isolate that point”.&lt;/li&gt;
  &lt;li&gt;Use the average path length to calculate a score which we can use to determine if something is an outlier. The smaller the average path length (i.e. we needed less cuts), the more likely a point is to be an outlier.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This algorithm is useful for a few reasons:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Typically outlier detection algorithms work by profiling what is “normal” and then finding deviations from that. This isn’t required when using isolation forests&lt;/li&gt;
  &lt;li&gt;It’s fast (and scales fairly well)&lt;/li&gt;
  &lt;li&gt;It doesn’t require a distance metric&lt;/li&gt;
  &lt;li&gt;There is an &lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html&quot;&gt;implementation&lt;/a&gt; of it in the popular python ML library scikit-learn&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;Anomaly detection is a task that shows up in many different applications. It’s typically used when “we want to find something interesting but we don’t know what that is”. In order to find the truly interesting data points, you need to add context by combining them with other information. This is especially important if you want to take an action (e.g. cancelling someone’s credit card) based on anomalous activity.&lt;/p&gt;

&lt;h3 id=&quot;other-resources&quot;&gt;Other resources&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/a-brief-overview-of-outlier-detection-techniques-1e0b2c19e561&quot;&gt;A brief overview of outlier detection techniques&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.floydhub.com/introduction-to-anomaly-detection-in-python/&quot;&gt;Introduction to anomaly detection in python&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Outlier (or anomaly) detection is the technical term for “finding weird stuff”. It’s used in a wide variety of applications including malware detection and looking for credit card fraud. For example, if you live in Ottawa but your credit card was used to buy a gaming console in Boise, Idaho (without any other purchases) that would be anomalous. Outlier detection is related to clustering. In clustering we are trying to find the groups of related data. In outlier detection we are trying to find the points that don’t belong to any groups. There are three different categories of outliers</summary></entry><entry><title type="html">N is for Natural Language Processing (NLP)</title><link href="https://abcsofdatascience.ca/blog/n-is-for-natural-language-processing" rel="alternate" type="text/html" title="N is for Natural Language Processing (NLP)" /><published>2020-10-18T00:00:00-05:00</published><updated>2020-10-18T00:00:00-05:00</updated><id>https://abcsofdatascience.ca/blog/n-is-for-natural-language-processing</id><content type="html" xml:base="https://abcsofdatascience.ca/blog/n-is-for-natural-language-processing">&lt;p&gt;Natural Language Processing (NLP) is a huge area within data science. It’s so huge that this blog will barely scratch the surface and will just give you a flavour of the kinds of things people try to use NLP for. As you might guess, the goal of NLP is to try and gain insights and information from language (either spoken or text). Text data can come from a wide variety of sources such as tweets, news articles, or transcripts of speech-to-text. NLP is used in a lot of applications, including&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Autocorrect&lt;/li&gt;
  &lt;li&gt;Chatbots and virtual assistants (e.g. Siri or Alexa)&lt;/li&gt;
  &lt;li&gt;Language translation (e.g. Google translate)&lt;/li&gt;
  &lt;li&gt;Document summarization&lt;/li&gt;
  &lt;li&gt;Text classification (e.g. is this email spam or not?)&lt;/li&gt;
  &lt;li&gt;Sentiment analysis (e.g. is this movie review positive or negative?)&lt;/li&gt;
  &lt;li&gt;Grouping documents&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In recent years, machine learning (&lt;a href=&quot;(/blog/d-is-for-deep-learning)&quot;&gt;deep learning&lt;/a&gt; in particular) has become increasingly popular within NLP but there are still a number of non-ML based techniques.&lt;/p&gt;

&lt;h3 id=&quot;language-models&quot;&gt;Language models&lt;/h3&gt;

&lt;p&gt;If you’ve ever tried to learn a new language, you probably know that languages are &lt;strong&gt;hard&lt;/strong&gt;. There are lots of weird rules (i.e. grammar) and there are even more exceptions to those rules. Language also changes depending on the context. For example, the language used in academic papers is very different from tweets. So given a &lt;strong&gt;corpus&lt;/strong&gt; (the technical term for a bunch of data such as documents) we want to learn how language is used within that set of documents. A document could be a tweet, an academic paper, an email etc. It’s nearly impossible to code all of the grammatical rules ahead of time, so we try to use NLP techniques to model language as it’s used in that corpus. The goal of this is not to relearn grammar, but give a better footing for the task that we really care about (e.g. sentiment analysis).&lt;/p&gt;

&lt;p&gt;A common practice in many NLP tasks is to use a &lt;strong&gt;language model&lt;/strong&gt; which lets us learn how specific words are used in a corpus. For example, words such as “the” or “and” occur much more frequently than say “lagniappe”. To train a language model, we take a bunch of text and then try to predict the next word. If we have the sentence “I have a golden retriever and she is the best” we want to use the previous words to predict the next word.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Given “I”, predict “have”&lt;/li&gt;
  &lt;li&gt;Given [“I”, “have”], predict “a”&lt;/li&gt;
  &lt;li&gt;Given [“I”, “have”, “a”], predict “golden”&lt;/li&gt;
  &lt;li&gt;Continue until you’ve predicted the number of words in the sentence&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We repeat this process and compare how our predictions match the actual text to improve the model. At the end of this we will have a predictive model for how different words are used in practice (e.g. “the” is much more likely than “pizza”). This is obviously a challenging task (and the model will often be wrong). Fortunately we can train models on huge amounts of text (e.g. wikipedia). We don’t even need extra labels since we already know what the next word is in a given sentence! In practice we can use language models that have already been trained so we don’t need to train a new language model on wikipedia for every task.  Language models are typically used as the starting point for other &lt;strong&gt;downstream tasks&lt;/strong&gt; such as text classification. In some cases they are used directly in applications like predictive text/autocorrect on your phone. The benefit and downside of language models is that they model how a language is used. This means that if enough people type something incorrectly it’s possible that the model will start suggesting the incorrect version. How a language model performs (which will then affect downstream task performance) is typically dependent on the amount of preprocessing done (more on that later).&lt;/p&gt;

&lt;h3 id=&quot;finding-spam-emails&quot;&gt;Finding spam emails&lt;/h3&gt;

&lt;p&gt;Let’s imagine we want to train a model to predict if an email is spam or not spam (ham). This is an example of a &lt;strong&gt;text classification&lt;/strong&gt; problem.&lt;/p&gt;

&lt;p&gt;Regular email:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Hey,&lt;br /&gt;&lt;/p&gt;

  &lt;p&gt;Want to grab lunch today? There’s a new taco truck downtown that looks great :)&lt;br /&gt;&lt;/p&gt;

  &lt;p&gt;-Alice&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Spam email:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Dear valued customer,&lt;br /&gt;&lt;/p&gt;

  &lt;p&gt;Your invoice is attached. In order to see your purchase history click &lt;a href=&quot;/images/n_is_for_nlp/click_me.jpg&quot;&gt;here&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;

  &lt;p&gt;Sincerely,&lt;br /&gt;
A totally legitimate business&lt;br /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;First we need to turn the corpus of emails into a format that our machine learning model can understand (i.e. numbers). This is called &lt;strong&gt;vectorization&lt;/strong&gt;. The simplest thing we could do is to count how often each word appears in each document. Unsurprisingly, this is called count vectorization. This gives us a &lt;strong&gt;word-document matrix&lt;/strong&gt; where each row corresponds to a document and each column corresponds to a word. The values in the matrix are how often each word occurred in a given document. As an example, let’s say we have the following three short emails (documents):&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The boss wants the report by Friday.&lt;/li&gt;
  &lt;li&gt;Pizza half price! This Friday only!&lt;/li&gt;
  &lt;li&gt;I ordered the pizza for the party.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Our word-document matrix would look something like this (for brevity not all words are included)&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;the&lt;/th&gt;
      &lt;th&gt;pizza&lt;/th&gt;
      &lt;th&gt;report&lt;/th&gt;
      &lt;th&gt;…&lt;/th&gt;
      &lt;th&gt;friday&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Doc 1&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;…&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Doc 2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;…&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Doc 3&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;…&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The columns are known as the &lt;strong&gt;vocabulary&lt;/strong&gt; since it is the unique set of words occurring in all documents. As you might imagine this matrix could get very big if there is a big vocabulary (and lots of documents). However, the matrix will be &lt;strong&gt;sparse&lt;/strong&gt; (mostly filled with zeroes) since most words will not appear in most documents. Fortunately, computer scientists have lots of ways to deal with sparse matrices so this is not a problem in practice.&lt;/p&gt;

&lt;p&gt;You might notice that the word columns aren’t in the same order as the words in the original documents. We call this a &lt;strong&gt;bag-of-words model&lt;/strong&gt; since we throw out all word ordering. Using a bag-of-words model means that we lose some information but it’s much faster computationally and it works surprisingly well in practice. Of course there are some applications (e.g. the language models described above) where order does matter.&lt;/p&gt;

&lt;p&gt;Count vectorization is very simple where we just count how often a word appears in a document. But how do we figure out what the words are? What if words are slightly different (e.g. “Pizza” and “pizza”)?&lt;/p&gt;

&lt;h3 id=&quot;preprocessing&quot;&gt;Preprocessing&lt;/h3&gt;

&lt;p&gt;Preprocessing is a catch-all term for anything we do to text before passing it into a model (including vectorization). You can potentially drastically improve the performance of your model by using more sophisticated preprocessing techniques. That being said, it’s often worth trying the simple things first!&lt;/p&gt;

&lt;h5 id=&quot;tokenization&quot;&gt;Tokenization&lt;/h5&gt;

&lt;p&gt;Tokenization is where we split some text into tokens (e.g. words). Taking a sentence and splitting it into words seems simple enough right? It’s easy enough to split a sentence on spaces and then use the resulting words. There are also more sophisticated tokenization techniques which will split within words (e.g. turning #datascience into “#” and “datascience”). This is also related to &lt;strong&gt;chunking&lt;/strong&gt; where you try to find the sentence boundaries in large pieces of text. How you tokenize a sentence also depends on the language. For example, a language like German which has a tendency to make new words by combining a bunch of existing words. You might want to split the new word into its original components.&lt;/p&gt;

&lt;p&gt;Related to tokenization is the notion of &lt;strong&gt;n-grams&lt;/strong&gt;. These are sequences of tokens which have n elements. For example, if we split the sentence “the dog loves treats” into bigrams (n=2) we would have&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[(“the”, “dog”), (“dog”, “loves”), (“loves”, “treats”)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This lets you capture a little more context around each word. Once we have n-grams we can just do count vectorization like we did above. Instead of columns corresponding to words (unigrams) they will correspond to n-grams. This means that our matrix is not “how often did this word appear in this document” it is “how often did this &lt;em&gt;sequence&lt;/em&gt; of words appear in this document”.&lt;/p&gt;

&lt;p&gt;It’s worth noting that you don’t need to have traditional language to tokenize. For example you could take file paths “/this/is/a/file/path” and split it into individual files/directories ([“this”, “is”, “a”, “file”, “path”]). Once you have tokens you can apply a wide range of NLP techniques.&lt;/p&gt;

&lt;h5 id=&quot;lowercasing-all-tokens&quot;&gt;Lowercasing all tokens&lt;/h5&gt;

&lt;p&gt;A really common (and easy) preprocessing step is to make everything lowercase. This means that “Friday” and “friday” are not treated as two separate tokens.&lt;/p&gt;

&lt;h5 id=&quot;stop-word-removal&quot;&gt;Stop word removal&lt;/h5&gt;

&lt;p&gt;Stop words are words that occur very frequently in a given language/corpus. In English these are words such as “the”, “and”, “they” (though there is no definitive list of stopwords). In many cases we want to filter out stop words since they don’t carry much information. This is more useful in tasks like text classification. In other cases such as automated translation you will need to keep stop words.&lt;/p&gt;

&lt;h5 id=&quot;stemminglemmatization&quot;&gt;Stemming/lemmatization&lt;/h5&gt;

&lt;p&gt;Stemming and lemmatization are used to help normalize text. There are many forms of words that all have the same base. For example, “the dog barks/barked/is barking” are all semantically similar. If we are training a model (say a language model) “barks”, “barked”, “barking” will all be treated as separate tokens. To make it easier we would like to normalize all of those tokens to “bark”, giving us the sentence “the dog bark”. Stemming turns a word into its base (e.g. barked to bark),  using language specific rules for removing prefixes or suffixes.  However, there are many edge cases so it’s not 100% effective. This is done Lemmatization is a more sophisticated form of stemming and normalizes words into their true base (e.g. normalizing “was” to “be”). Again, this is based on language specific rules (and a bunch of lookup tables).&lt;/p&gt;

&lt;h5 id=&quot;minimum-termdocument-frequency&quot;&gt;Minimum term/document frequency&lt;/h5&gt;

&lt;p&gt;If we keep every word that shows up in any of our documents our vocabulary size will be enormous. In order to reduce the vocabulary size, one common trick is to only keep words/tokens that only occur more than N times. For example, if there is a document that contains the token “maewpfaefamefaef” it’s pretty unlikely that it’s going to show up frequently. So we can just get rid of this by saying “don’t include words that occur less than 5 times”. Similarly, we can also drop words if they show up in less than N (e.g. 5) documents. For example, if we had a document that was “&lt;a href=&quot;https://en.wikipedia.org/wiki/Buffalo_buffalo_Buffalo_buffalo_buffalo_buffalo_Buffalo_buffalo&quot;&gt;Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo&lt;/a&gt;” the word buffalo occurs more than 5 times. But if that was the only document that buffalo appeared in, we probably want to drop buffalo from our vocabulary.&lt;/p&gt;

&lt;h3 id=&quot;beyond-simple-counting&quot;&gt;Beyond simple counting&lt;/h3&gt;

&lt;p&gt;If we just count how often words appear in documents, there are going to be words (e.g. “the”) which occur frequently but don’t contain much information. How do we deal with the fact that some words convey more information than others? One way to do this is to weight our counts using “Term Frequency - Inverse Document Frequency” (TF-IDF). The intuition behind TF-IDF is as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;If a word appears frequently in most documents in the corpus it probably doesn’t give much information. So we should give those words less weight since they don’t mean as much.&lt;/li&gt;
  &lt;li&gt;If a word appears frequently in a small number of documents then it probably has more information. For example, the word “inheritance” might appear more often than you would expect in spam emails, but not in most normal emails. We should give these words more weight.&lt;/li&gt;
  &lt;li&gt;If a word doesn’t occur that frequently, then it doesn’t really give useful information. For example, if the word “oxymoron” occurred 10 times in our email corpus it doesn’t really help us distinguish between spam/not spam.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In &lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html&quot;&gt;TF-IDF vectorization&lt;/a&gt;, we do count vectorization as we did before then apply one additional step. This extra step is just multiplying the counts by the weight of each word. Using this weighting will help our model distinguish more easily between spam and not/spam.&lt;/p&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;This was just a brief introduction to some of the concepts used in NLP. There are many things that can make NLP more complicated in practice such as dealing with multiple languages in the same corpus. NLP techniques can be a really powerful toolset to have at your disposal and they don’t just apply to traditional text data. If you have data that you can tokenize, then you can apply all of the techniques described above. If you want to dive into some NLP projects I recommend starting with &lt;a href=&quot;https://www.fast.ai/2019/07/08/fastai-nlp/&quot;&gt;this course&lt;/a&gt; from fast.ai.&lt;/p&gt;

&lt;h3 id=&quot;other-resources&quot;&gt;Other resources&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.fast.ai/2019/07/08/fastai-nlp/&quot;&gt;A Code-First Introduction to Natural Language Processing&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://gretchenmcculloch.com/book/&quot;&gt;Because Internet: Understanding the New Rules of Language&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=oi0JXuL19TA&amp;amp;list=PL8dPuuaLjXtO65LeD2p4_Sb5XQ51par_b&amp;amp;t=0s&quot;&gt;Natural Language Processing: Crash Course AI #7&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/tokenization-for-natural-language-processing-a179a891bad4&quot;&gt;Tokenization strategies&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Natural Language Processing (NLP) is a huge area within data science. It’s so huge that this blog will barely scratch the surface and will just give you a flavour of the kinds of things people try to use NLP for. As you might guess, the goal of NLP is to try and gain insights and information from language (either spoken or text). Text data can come from a wide variety of sources such as tweets, news articles, or transcripts of speech-to-text. NLP is used in a lot of applications, including</summary></entry><entry><title type="html">M is for Munging Data</title><link href="https://abcsofdatascience.ca/blog/m-is-for-munging-data" rel="alternate" type="text/html" title="M is for Munging Data" /><published>2020-10-10T00:00:00-05:00</published><updated>2020-10-10T00:00:00-05:00</updated><id>https://abcsofdatascience.ca/blog/m-is-for-munging-data</id><content type="html" xml:base="https://abcsofdatascience.ca/blog/m-is-for-munging-data">&lt;p&gt;You might think that data scientists spend most of their time training machine learning models. In fact most of the time (60%-80%) spent on a data science project is acquiring and preparing the data. In the case of supervised learning problems this also includes &lt;a href=&quot;/blog/l-is-for-labelling-data&quot;&gt;getting labels&lt;/a&gt;. This process of preparing data is often referred to as data munging or data wrangling. Data wrangling typically includes a number of tasks:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Getting and cleaning the data&lt;/li&gt;
  &lt;li&gt;Selecting features&lt;/li&gt;
  &lt;li&gt;Engineering features&lt;/li&gt;
  &lt;li&gt;Transforming the data&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To paraphrase the bodybuilder &lt;a href=&quot;https://www.youtube.com/watch?v=4UlgXIL0-3g&quot;&gt;Ronnie Coleman&lt;/a&gt; “everybody wants to be a data scientist but don’t nobody want to munge no ugly-ass data!”. While it isn’t the most glamorous part of data science, good data preparation is critical for having models that perform well. Properly cleaned data and good features can give better performance than trying to tweak the model itself.&lt;/p&gt;

&lt;h3 id=&quot;getting-data&quot;&gt;Getting data&lt;/h3&gt;

&lt;p&gt;Datasets can come in a wide variety of formats but there are a few common ways of accessing them:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Spreadsheets: These are typically excel spreadsheets or CSV (comma separated value) files. These can be easily read in using something like the pandas library in python.&lt;/li&gt;
  &lt;li&gt;Web APIs: Many data sources provide an API for fetching data, and you can interact with these APIs using python’s requests module. If you need to scrape the website itself, the beautifulsoup module is extremely helpful.&lt;/li&gt;
  &lt;li&gt;Databases: Data scientists will often interact with their data in databases using SQL.&lt;/li&gt;
  &lt;li&gt;Unstructured data: All of the formats above provide data in some structured manner. Unfortunately, this is not always possible and you may need to use &lt;a href=&quot;https://xkcd.com/208/&quot;&gt;regular expressions&lt;/a&gt; and other techniques to parse the data.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;cleaning-and-transforming-data&quot;&gt;Cleaning and transforming data&lt;/h3&gt;

&lt;p&gt;When you are first looking at a new dataset it is extremely important that you &lt;strong&gt;look at your data&lt;/strong&gt;! A fancier term for looking at your data is Exploratory Data Analysis (EDA). Once you have an environment set up, what does EDA look like in practice?&lt;/p&gt;

&lt;h5 id=&quot;looking-for-missing-data&quot;&gt;Looking for missing data&lt;/h5&gt;

&lt;p&gt;Real world data is messy and there can be mistakes or missing data. You might need to infer missing values or drop rows with too much missing data. How you infer missing data depends on the data type and what works best for your problem. For example, if you have a field with missing numbers, you could fill in the missing value with the average or just put 0. You also need to check if there is a pattern to which rows/fields have missing values. If there is a pattern (i.e. it isn’t random) you will need to compensate for that as well.&lt;/p&gt;

&lt;h5 id=&quot;visualizing-data&quot;&gt;Visualizing data&lt;/h5&gt;

&lt;p&gt;Data visualization is an invaluable tool when exploring data. Below are some questions that we are typically trying to answer by visualizing data&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;What do the feature distributions look like? This could be as simple as answering the question “Do some values occur very frequently?”. This might be answered by making a histogram of your data.&lt;/li&gt;
  &lt;li&gt;Do some features correlate with one another? For example in a census dataset, the neighbourhood someone lives in will typically correlate with household income.&lt;/li&gt;
  &lt;li&gt;Are there big patterns that jump out in the data? It might be hard to see these patterns (e.g. lots of duplicate points) when looking at a spreadsheet, but when plotted are very obvious.&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;transforming-data&quot;&gt;Transforming data&lt;/h5&gt;

&lt;p&gt;It is pretty common to have to tweak your data into a format that a machine learning algorithm expects. Let’s imagine we have a dataset about video games&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Title&lt;/th&gt;
      &lt;th&gt;Metacritic score (/100)&lt;/th&gt;
      &lt;th&gt;IGN score (/10)&lt;/th&gt;
      &lt;th&gt;Genre&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;The Legend of Zelda: Breath of the Wild&lt;/td&gt;
      &lt;td&gt;97&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;Action-adventure&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Untitled Goose Game&lt;/td&gt;
      &lt;td&gt;81&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;Puzzle&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;James Bond 007: Nightfire&lt;/td&gt;
      &lt;td&gt;80&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;First person shooter&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Here we have two numeric columns (metacritic score and IGN score) but they are on different scales. One goes from 0-100 while the other is from 0-10. Some ML algorithms assume that all of the features are on the same scale, so we would need to &lt;strong&gt;normalize&lt;/strong&gt; these features. This could mean converting all the numbers so that they are between 0 and 1 (e.g. 8/10 becomes 0.8).&lt;/p&gt;

&lt;p&gt;Many ML algorithms assume that all of your input is numeric. How do we convert the genre field (which is a &lt;strong&gt;categorical value&lt;/strong&gt;) to a numeric one? One simple way to do this is known as &lt;strong&gt;one-hot encoding&lt;/strong&gt;. This just means representing all categories as a vector where there is a 1 if the category matches and a 0 if it doesn’t. Using our example above we have:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;Action-adventure&lt;/th&gt;
      &lt;th&gt;Puzzle&lt;/th&gt;
      &lt;th&gt;First person shooter&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;The Legend of Zelda: Breath of the Wild&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Untitled Goose Game&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;James Bond 007: Nightfire&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;So instead of having the genre value of “Puzzle” for Untitled Goose Game, we would have [0, 1, 0]. A one-hot encoding is a very simple version of an &lt;a href=&quot;/blog/e-is-for-embeddings&quot;&gt;embedding&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If we have text data there are many techniques that we can use which I’ll talk about in the next blog N is for Natural Language Processing.&lt;/p&gt;

&lt;h5 id=&quot;common-tools-for-eda&quot;&gt;Common tools for EDA&lt;/h5&gt;

&lt;p&gt;&lt;a href=&quot;https://jupyter.org/&quot;&gt;Jupyter notebooks&lt;/a&gt; are a popular environment for doing EDA, since it provides an interactive development environment where you can see output (e.g. plots) inline with your code. The &lt;a href=&quot;https://pandas.pydata.org/&quot;&gt;pandas&lt;/a&gt; module in python is very commonly used for data munging tasks and has a lot of useful utilities. I’ll talk more about the pandas library in a later blog post.&lt;/p&gt;

&lt;h3 id=&quot;feature-selection-and-engineering&quot;&gt;Feature selection and engineering&lt;/h3&gt;

&lt;p&gt;When preparing data for training a model you need to figure out which features in your data will be most relevant for the problem you are trying to solve (e.g. classification). There may be features that would be helpful that don’t exist in the data as it comes in. Creating new features is referred to as feature engineering. Both feature selection and engineering require some expertise in the problem domain. One way to determine useful features if you have labels is to look at the features which are strongly correlated with those labels. Imagine you are trying to predict if a file is malware or benign. If there are attributes of that file that occur frequently when the file is malware and don’t occur when it is benign, that would be a useful feature. In the case of classification, you are looking for features that make it easy to discriminate between the classes. In the case of &lt;a href=&quot;/blog/c-is-for-clustering&quot;&gt;clustering&lt;/a&gt; you need to choose features that say “these points are similar if they have X in common”. Feature selection/engineering is more of an art than a science and can involve some trial and error.&lt;/p&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;Data munging is a crucial part of data science (and you could argue it’s the majority of data science). Properly cleaning and normalizing your data can have huge benefits for the downstream task you are trying to solve. It is challenging and sometimes frustrating (looking at you regular expressions!) task but is necessary to understand your data and train an effective model.&lt;/p&gt;

&lt;h3 id=&quot;other-resources&quot;&gt;Other resources&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://machinelearningmastery.com/what-is-data-preparation-in-machine-learning/&quot;&gt;More on data preparation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://lineardigressions.com/episodes/2019/10/1/whats-really-so-hard-about-feature-engineering&quot;&gt;What’s &lt;em&gt;really&lt;/em&gt; so hard about feature engineering&lt;/a&gt; (episode of the Linear Digressions podcast)&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">You might think that data scientists spend most of their time training machine learning models. In fact most of the time (60%-80%) spent on a data science project is acquiring and preparing the data. In the case of supervised learning problems this also includes getting labels. This process of preparing data is often referred to as data munging or data wrangling. Data wrangling typically includes a number of tasks:</summary></entry><entry><title type="html">L is for Labelling Data</title><link href="https://abcsofdatascience.ca/blog/l-is-for-labelling-data" rel="alternate" type="text/html" title="L is for Labelling Data" /><published>2020-10-09T00:00:00-05:00</published><updated>2020-10-09T00:00:00-05:00</updated><id>https://abcsofdatascience.ca/blog/l-is-for-labelling-data</id><content type="html" xml:base="https://abcsofdatascience.ca/blog/l-is-for-labelling-data">&lt;p&gt;Having high quality labelled data is critical to training useful supervised learning models. This labelled training data needs to be directly relevant to the real world task you are trying to accomplish. Unfortunately, getting this labelled data can be very challenging in a lot of cases. This is for a couple reasons:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Labelling data is time intensive and also really boring/tedious. You often need hundreds or thousands of examples (or more!) so it takes a while to create a labelled training set. People are also incentivized/rewarded for training a useful ML model, as compared to labelling 1000s of points.&lt;/li&gt;
  &lt;li&gt;Having high quality labels often requires input from subject matter experts (whose time is valuable). For example, looking at x-ray images to label the images to have a given disease. This would require a medical professional who may not have the time required to label 1000s of images. Even if the data scientists themselves are doing the labelling, they are also typically highly paid professionals who may not have the time or commitment from management to spend hours labelling data.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;labelling-data-in-practice&quot;&gt;Labelling data in practice&lt;/h3&gt;

&lt;p&gt;Fortunately in the real world there are a few ways to make getting a labelled data set easier (not easy, but easier).&lt;/p&gt;

&lt;h5 id=&quot;pay-someone-else-to-do-it&quot;&gt;Pay someone else to do it&lt;/h5&gt;

&lt;p&gt;There are an increasing number of services where you can outsource your data labelling tasks. This includes Amazon’s &lt;a href=&quot;https://aws.amazon.com/sagemaker/groundtruth/&quot;&gt;SageMaker Ground Truth&lt;/a&gt; or Google’s &lt;a href=&quot;https://cloud.google.com/ai-platform/data-labeling/docs&quot;&gt;AI Platform Data Labelling Service&lt;/a&gt;. These services can work if you have a large budget. However, if the dataset you are trying to label is sensitive (e.g. medical records) or requires a large amount of subject matter expertise then you may not be able to use these services. Alternatively, just pay grad students to &lt;del&gt;label data&lt;/del&gt; do research.&lt;/p&gt;

&lt;h5 id=&quot;make-people-do-it-for-free&quot;&gt;Make people do it for free&lt;/h5&gt;

&lt;p&gt;You’ve probably needed to go through an image based captcha (e.g. “select all the pictures with traffic lights”) in order to log into a popular web service. If you’re a large organization or have a product that many people use, this can be an effective way to get people to label data for free.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/l_is_for_labelling_data/image_captcha.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;use-pretrained-models-or-datasets&quot;&gt;Use pretrained models or datasets&lt;/h5&gt;

&lt;p&gt;In some cases there may be publicly available models (e.g. &lt;a href=&quot;https://modelzoo.co/&quot;&gt;modelzoo&lt;/a&gt;) that have been trained on a similar task to the one you’re interested in. Similarly, there are publicly available labelled datasets (e.g. &lt;a href=&quot;https://www.kaggle.com/datasets&quot;&gt;kaggle datasets&lt;/a&gt;) that you could train your own model on. These datasets/models could come from academic researchers, companies such as Google, or open source projects. If the pretrained model gives acceptable results for your problem then great! It is worth thinking about how your dataset/task is different than the one the model was trained on. If your data diverges from the original dataset over time, then your model will be less accurate. This is known as &lt;strong&gt;model drift&lt;/strong&gt;. It’s possible to &lt;strong&gt;fine-tune&lt;/strong&gt; these pretrained models onto your dataset/labels to improve their performance for your task. I’ll talk about that in more detail in T is for Transfer Learning.&lt;/p&gt;

&lt;h5 id=&quot;using-unsupervised-learning&quot;&gt;Using unsupervised learning&lt;/h5&gt;

&lt;p&gt;Instead of labelling each point individually, you can often try &lt;a href=&quot;/blog/c-is-for-clustering&quot;&gt;clustering&lt;/a&gt; your data beforehand. This allows you to label groups of data instead of each individual point.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/l_is_for_labelling_data/dogs_and_cats_labels_smaller.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can build this clustering and labelling system yourself or use a labelling tool which does the same thing. Some of these include (most you need to pay for):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.platform.ai/&quot;&gt;platform.ai for images&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://prodi.gy/&quot;&gt;prodigy for text&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Snorkel has a &lt;a href=&quot;https://www.snorkel.ai/&quot;&gt;paid&lt;/a&gt; and a &lt;a href=&quot;https://www.snorkel.org/&quot;&gt;free&lt;/a&gt; product&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;data-augmentation&quot;&gt;Data augmentation&lt;/h5&gt;

&lt;p&gt;One of the reasons for having large training sets is to have many examples for the model to learn patterns. If you are trying to classify images of dogs you need to have many different pictures of different breeds. The pictures need to be from different angles (head on, side view, etc.) to be able to recognize the dogs in different scenarios. One way to increase the size of your dataset is to use &lt;strong&gt;data augmentation&lt;/strong&gt;. If you have a picture you can generate new labelled data by transforming the original image. This might include blurring, cropping, or rotating the image. Using this technique you can get multiple labelled data points for each image that was labelled by a human. Having multiple versions of an image with varying amounts of noise will make your model more robust as well. Data augmentation can be applied to other data types as well (e.g. text) but it’s slightly trickier than doing it with images.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/l_is_for_labelling_data/data_augmentation.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;active-learning&quot;&gt;Active learning&lt;/h5&gt;

&lt;p&gt;This is a technique where you can use a ML model to help you label your data. It works as follows:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Label a small amount of your training set (e.g. 10%)&lt;/li&gt;
  &lt;li&gt;Train a ML model on the labelled data you have&lt;/li&gt;
  &lt;li&gt;Use that ML model to predict labels for the other 90%. You can verify the labels for the predictions where the model was not very confident. You should also randomly check predictions where the model was confident just to check that it is working as intended.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Using this method can help speed up the data labelling process and give you a better model overall.&lt;/p&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;Getting labelled data can be one of the most challenging parts of training a supervised learning model. If you aren’t able to throw money at the problem (most of us aren’t!) then there are many techniques you can try to increase your productivity when labelling data such as using labelling tools or data augmentation. When using existing datasets and models make sure to think about the similarities and differences between that dataset and the problem you want to tackle.&lt;/p&gt;

&lt;h3 id=&quot;other-resources&quot;&gt;Other resources&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.kdnuggets.com/2017/06/acquiring-quality-labeled-training-data.html&quot;&gt;7 Ways to Get High-Quality Labeled Training Data at Low Cost&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=ANIw1Mz1SRI&quot;&gt;Active Learning - Computerphile&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Having high quality labelled data is critical to training useful supervised learning models. This labelled training data needs to be directly relevant to the real world task you are trying to accomplish. Unfortunately, getting this labelled data can be very challenging in a lot of cases. This is for a couple reasons:</summary></entry><entry><title type="html">K is for K-fold cross-validation</title><link href="https://abcsofdatascience.ca/blog/k-is-for-kfold-cross-validation" rel="alternate" type="text/html" title="K is for K-fold cross-validation" /><published>2020-10-05T00:00:00-05:00</published><updated>2020-10-05T00:00:00-05:00</updated><id>https://abcsofdatascience.ca/blog/k-is-for-kfold-cross-validation</id><content type="html" xml:base="https://abcsofdatascience.ca/blog/k-is-for-kfold-cross-validation">&lt;p&gt;In supervised learning problems we have data as well as a label (something we need to predict) for each data point. We typically refer to the combination of data + labels as &lt;strong&gt;training data&lt;/strong&gt;. You might remember from &lt;a href=&quot;(/blog/f-is-for-f1)&quot;&gt;F is for F1 score&lt;/a&gt; that there are two broad categories of supervised learning problems: classification and regression. In classification, we are trying to predict which class (or category) the input data belongs to. For example, this might be trying to predict if an email is spam or not spam. There are typically a small number of categories to choose from (e.g. spam/not spam). In regression problems, we are trying to predict a continuous variable (i.e. a number) such as the selling price of a house based on some features (e.g. square footage). Here we are not trying to pick from a small group of categories, but to get close to the actual number (where there can be infinitely possible values).&lt;/p&gt;

&lt;p&gt;In all supervised learning problems we want to know well our model is doing and how well it can predict things for unseen data.&lt;/p&gt;

&lt;h3 id=&quot;how-does-our-model-fit-the-data&quot;&gt;How does our model fit the data?&lt;/h3&gt;

&lt;p&gt;Lets focus on regression and take a further look at the housing example. For now assume we have a training set and some way to model our data. How do we know if our model is any good? Let’s take a look at our imaginary training data&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/k_is_for_kfold_cv/house_price_dataset.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As you can imagine, as the square footage of a house increases the price goes up. If our model tries to fit this data with a straight line then we get something like this&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/k_is_for_kfold_cv/house_price_underfit.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This line is an okay fit to the data but doesn’t really give accurate predictions. We say that this model &lt;strong&gt;underfits&lt;/strong&gt; the data or has &lt;strong&gt;high bias&lt;/strong&gt;. What we really want is a function that is much closer to all the points&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/k_is_for_kfold_cv/house_price_proper_fit.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is a pretty good fit to the data and seems to do what one would intuitively expect. The function doesn’t pass through every point but it’s pretty close to most of them. Our model could probably find some function that passes through every point&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/k_is_for_kfold_cv/house_price_overfit.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;While this function passes through every point it won’t work well on new data. We say that this model &lt;strong&gt;overfits&lt;/strong&gt; the data or has &lt;strong&gt;high variance&lt;/strong&gt;. This means that the model has two many parameters or there are not enough features in the training set. In this case it would probably be beneficial to add more features to the training set. For example, there is a huge difference in price between a 1000 square foot house in San Francisco versus Thunder Bay. So adding the location of the house might improve the model. In the case of the model that underfit the data, adding more features won’t help. We need to add more parameters to the model so that there is more room to try and fit the data.&lt;/p&gt;

&lt;h3 id=&quot;testing-our-model&quot;&gt;Testing our model&lt;/h3&gt;

&lt;p&gt;So how do we tell if our model is any good? Typically, the training data is broken into 2 parts. There is a &lt;strong&gt;training set&lt;/strong&gt; (about 70% of the data) and a &lt;strong&gt;test set&lt;/strong&gt; (the remaining 30%). Splitting the data into training/test sets is known as &lt;strong&gt;cross-validation&lt;/strong&gt;. As the name implies the parameters of the model are trained using the training set. The model is then fed in the data from the test set and the &lt;a href=&quot;https://abcsofdatascience.ca/blog/f-is-for-f1&quot;&gt;error&lt;/a&gt; is calculated. Using the error associated with both the training and the test sets you can diagnose if your model underfits, overfits, or is a good fit to the data.&lt;/p&gt;

&lt;p&gt;In addition to using a train/test split, people will often use a third split: the &lt;strong&gt;validation set&lt;/strong&gt; (sometimes called a development set). Let’s break down what each of those sets are used for:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Training set: As you would expect, this is the data used to actually train the model. As you might &lt;a href=&quot;/blog/g-is-for-gradient-descent&quot;&gt;recall&lt;/a&gt;, training a model just means learning which feature weights give you the best predictions (i.e. match your labels as closely as possible).&lt;/li&gt;
  &lt;li&gt;Validation set: This is used for learning the best set of &lt;strong&gt;hyperparameters&lt;/strong&gt;. Hyperparameters are knobs you can turn for the model itself (e.g. learning rate or mini-batch size). In the training set where we are trying to answer “how can I weight these features to get the best predictions?”. In the validation set we are trying to answer “how can I adjust the model itself to give the best predictions when training?”&lt;/li&gt;
  &lt;li&gt;Test set: We want to test our model on data that it has not seen in order to see if it is overfitting/underfitting. We never update the model (or hyperparameters) on the test set and we just make predictions on it using the trained model.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;k-fold-cross-validation&quot;&gt;K-fold cross-validation&lt;/h3&gt;

&lt;p&gt;If you split your data into training/validation/test sets using a 60/20/20 split then that means you can only use 60% of your available data for model training and the validation set is a fixed 20% of the data. In cases where you don’t have a lot of training data (which is often) this extra 20% can make a big difference! K-fold cross-validation lets you get the benefits of having a validation set without having to hold out 20% of your training data.&lt;/p&gt;

&lt;p&gt;In k-fold cross-validation you have a training set and what I will call the true test set. This test set is exactly what is described above: unseen data used to test predictions. The k-fold part comes from taking your training set and splitting it into k groups (or folds). The value of k can be any number that you choose but as an example let’s say k is 5. You would split your training set into 5 folds and train the model 5 times. The first time you train, the first fold is used as the “test” set and the remaining 4 folds are used as a training set. The second time you train, you use the second fold as a “test” set and the other 4 folds are used as a training set. You repeat this process until you’ve used each fold as a test set. After you have fully trained the model you test your predictions on the true test set as before. This process is illustrated in the image below (taken from the wonderful scikit-learn documentation). This process can be more computationally expensive since you need to train k models instead of 1 but also means you don’t need a held out validation set.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/k_is_for_kfold_cv/sklearn_kfold_cross_validation.png&quot; alt=&quot;&quot; title=&quot;Taken from https://scikit-learn.org/stable/_images/grid_search_cross_validation.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;How do you choose a value for k? There are some different methods for choosing k since a wrong value can lead you to be overconfident in your model:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Pick k=10. This is a decent starting point (or k=5) and has been shown to work in a wide variety of applications.&lt;/li&gt;
  &lt;li&gt;Pick a value of k so that each fold gives a statistically representative sample size&lt;/li&gt;
  &lt;li&gt;Let k be the number of points you have in your training set so that each point can be used in the hold out test set. This is also referred to as leave-one-out cross-validation.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;Evaluating the performance of supervised learning models is critical and you want to be sure that your models aren’t overfitting/underfitting the training data. Cross-validation is a technique that should be used in all supervised learning applications to check that your model is behaving as expected. It is also important to keep in mind that even if your model generalizes well to the test set, it may not generalize to the real world if your training data is not truly representative.&lt;/p&gt;

&lt;h3 id=&quot;other-resources&quot;&gt;Other resources&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://scikit-learn.org/stable/modules/cross_validation.html&quot;&gt;Cross-validation explanation in scikit-learn&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://machinelearningmastery.com/k-fold-cross-validation/&quot;&gt;K-fold cross-validation explanation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">In supervised learning problems we have data as well as a label (something we need to predict) for each data point. We typically refer to the combination of data + labels as training data. You might remember from F is for F1 score that there are two broad categories of supervised learning problems: classification and regression. In classification, we are trying to predict which class (or category) the input data belongs to. For example, this might be trying to predict if an email is spam or not spam. There are typically a small number of categories to choose from (e.g. spam/not spam). In regression problems, we are trying to predict a continuous variable (i.e. a number) such as the selling price of a house based on some features (e.g. square footage). Here we are not trying to pick from a small group of categories, but to get close to the actual number (where there can be infinitely possible values).</summary></entry><entry><title type="html">J is for Jaccard metric</title><link href="https://abcsofdatascience.ca/blog/j-is-for-jaccard" rel="alternate" type="text/html" title="J is for Jaccard metric" /><published>2020-09-27T00:00:00-05:00</published><updated>2020-09-27T00:00:00-05:00</updated><id>https://abcsofdatascience.ca/blog/j-is-for-jaccard</id><content type="html" xml:base="https://abcsofdatascience.ca/blog/j-is-for-jaccard">&lt;p&gt;In &lt;a href=&quot;/blog/e-is-for-embeddings&quot;&gt;previous blogs&lt;/a&gt; we’ve talked about choosing a distance measure as a way of saying “these two things are close if …”. One useful measure is Jaccard similarity/distance since it measures the similarity between two sets. This is useful if you have a lot of categorical variables (i.e. ones that don’t have any inherent ordering). For example, two people are probably similar if they have the same sets of interests/hobbies. The Jaccard similarity of two sets is just the size of the intersection divided by the size of the union. Or put visually:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/j_is_for_jaccard/jaccard_small.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;jaccard-similaritydistance&quot;&gt;Jaccard similarity/distance&lt;/h3&gt;

&lt;p&gt;As a more concrete example let’s imagine that we have collected a list of people’s favourite pizza toppings and we want to find which people are most similar.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Kara likes pepperoni, mushrooms, and green pepper&lt;/li&gt;
  &lt;li&gt;Zach likes ham, pineapple, and jalapeno peppers&lt;/li&gt;
  &lt;li&gt;Rodney also likes pepperoni, mushrooms, and green peppers&lt;/li&gt;
  &lt;li&gt;Sophie likes olives, ham, pepperoni, and mushrooms&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So what are the Jaccard similarities between these people?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Kara and Rodney like exactly the same toppings so their similarity is 1&lt;/li&gt;
  &lt;li&gt;Rodney and Zach have nothing in common so their similarity is 0&lt;/li&gt;
  &lt;li&gt;Kara and Sophie have some things in common but Sophie enjoys more toppings. Their similarity is 0.4 ([pepperoni, mushrooms]/[pepperoni, mushrooms, olives, ham])&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You’ll notice that two sets that are exactly the same have a similarity of 1. To change this into a distance we just to&lt;/p&gt;

&lt;p&gt;$D_{Jaccard} = 1 - similarity$&lt;/p&gt;

&lt;p&gt;Now Kara and Rodney have a distance of 0, while Rodney and Zach are a distance of 1 (which in this case is as far apart as you can be).&lt;/p&gt;

&lt;h3 id=&quot;hellinger-distance&quot;&gt;Hellinger distance&lt;/h3&gt;

&lt;p&gt;You might have noticed that Jaccard distance doesn’t take into account how frequently the items in the set occur. If counts do matter for your problem, then you will want to use &lt;a href=&quot;https://en.wikipedia.org/wiki/Hellinger_distance&quot;&gt;Hellinger distance&lt;/a&gt;. Let’s imagine you have some data on the number of times people have read a given book:&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Harry Potter&lt;/td&gt;
      &lt;td&gt;Hello World&lt;/td&gt;
      &lt;td&gt;The Hobbit&lt;/td&gt;
      &lt;td&gt;The Great Gatsby&lt;/td&gt;
      &lt;td&gt;Trick Mirror&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Marie&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Jordan&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Sarah&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Patrick&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Marie, Jordan, and Patrick have all read Harry Potter which would make them similar under Jaccard similarity. However, Marie and Patrick are probably more similar since they both read it multiple times. Hellinger distance takes this into account. I’ll try to give you some intuition for how it does this.&lt;/p&gt;

&lt;p&gt;Imagine that each set of counts is generated by some weighted multi-sided die (that is different for each person). When we roll Maries die, it is more likely to come up with Harry Potter and less likely to come up with Hello World. The opposite is true for Sarah’s die, which is more likely to come up with Hello World and less likely to come up with Harry Potter. We calculate what the weights of these dice look like (these are called multinomial distributions).&lt;/p&gt;

&lt;p&gt;We can then measure the mutual likelihood of these distributions. This just means “what is the probability of Jordans counts occuring using Maries die (and vice versa)”. If there is a high probability that Jordans counts occurred using Maries die, then Marie and Jordan should be considered close. If it is unlikely that Sarah’s counts occurred using Maries die, then Marie and Sarah should be pushed apart.&lt;/p&gt;

&lt;p&gt;Hellinger distance is particularly useful if you have a bunch of text. You can consider two documents similar if you have the same words occurring at similar frequencies.&lt;/p&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;Jaccard and Hellinger are both very useful distance measures that can be used in dimension reduction and embeddings. If counts matter, use Hellinger, otherwise use Jaccard distance.&lt;/p&gt;

&lt;h3 id=&quot;other-resources&quot;&gt;Other resources&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=OtVR_ZnXLu4&amp;amp;list=PLGVZCDnMOq0pHVE3SB0ecki__VMncQPKo&amp;amp;index=41&amp;amp;t=0s&quot;&gt;Embed all the things - John Healy (talk from Pydata Los Angeles 2019)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">In previous blogs we’ve talked about choosing a distance measure as a way of saying “these two things are close if …”. One useful measure is Jaccard similarity/distance since it measures the similarity between two sets. This is useful if you have a lot of categorical variables (i.e. ones that don’t have any inherent ordering). For example, two people are probably similar if they have the same sets of interests/hobbies. The Jaccard similarity of two sets is just the size of the intersection divided by the size of the union. Or put visually:</summary></entry><entry><title type="html">I is for Interpretability</title><link href="https://abcsofdatascience.ca/blog/i-is-for-interpretability" rel="alternate" type="text/html" title="I is for Interpretability" /><published>2020-09-27T00:00:00-05:00</published><updated>2020-09-27T00:00:00-05:00</updated><id>https://abcsofdatascience.ca/blog/i-is-for-interpretability</id><content type="html" xml:base="https://abcsofdatascience.ca/blog/i-is-for-interpretability">&lt;p&gt;Machine learning models are increasingly used to make decisions in everyday life, and as we’ve discussed before, they can be 
&lt;a href=&quot;/blog/b-is-for-bias&quot;&gt;far from perfect&lt;/a&gt;. In addition to being able to appeal the decisions made by a model, 
it is critical to be able to interpret their predictions. Doing this in practice opens up a huge number of questions including&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;What does interpretability mean?&lt;/li&gt;
  &lt;li&gt;Why does it matter?&lt;/li&gt;
  &lt;li&gt;How do we actually interpret models in practice?&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pinning-down-interpretability&quot;&gt;Pinning down interpretability&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1606.03490.pdf&quot;&gt;This paper&lt;/a&gt; titled &lt;em&gt;The Mythos of Model Interpretability&lt;/em&gt; by &lt;a href=&quot;https://twitter.com/zacharylipton&quot;&gt;Zachary Lipton&lt;/a&gt;, gives a really good overview of what people mean when they say “interpretability”. He argues that interpretability is not just one monolithic concept, but a number of distinct ideas. It’s worth looking at the different reasons we try to interpret ML models.&lt;/p&gt;

&lt;h5 id=&quot;do-we-trust-the-model&quot;&gt;Do we trust the model?&lt;/h5&gt;

&lt;p&gt;Using a ML model to automate some task often requires a human giving up control.  We often care about the kinds of predictions the model gets wrong and if there is a pattern to these incorrect predictions. For example, many facial recognition models have significantly worse performance on people of color than white people. In this case using the model with this bias would be unacceptable. However, there may be other cases where the model gets things wrong in the same way humans do. In that case it may be acceptable to use the model to make predictions.&lt;/p&gt;

&lt;h5 id=&quot;can-we-use-the-model-to-learn-something-about-the-world&quot;&gt;Can we use the model to learn something about the world?&lt;/h5&gt;

&lt;p&gt;Researchers will often look at which features of a trained model are most important in making predictions. Imagine you have a model which is trying to predict if a patient has lung cancer. As input features you have the number of years the patient has smoked cigarettes, and the number of years they have chewed bubble gum. Of course, smoking correlates much more with lung cancer than chewing bubble gum and that feature would have a much greater importance. It’s important to keep in mind that correlation does not equal causation. For example, [the per capita consumption of margarine in the US strongly correlates with the divorce rate in Maine(http://www.tylervigen.com/spurious-correlations)]. Researchers can then use these important features to create experiments to test if those correlations are causal.&lt;/p&gt;

&lt;h5 id=&quot;will-the-ml-model-help-a-human-make-better-predictions&quot;&gt;Will the ML model help a human make better predictions?&lt;/h5&gt;

&lt;p&gt;A common use of ML models is to help a human make a more informed decision. Imagine you work in cybersecurity and are tasked with finding malware. Fortunately, you have a ML model which determines if files are malware or benign. The important features used by the model (e.g. is the code heavily obfuscated?) can help you make a better decision in your investigation. ML models can also help you find similar examples to give you more context when triaging the file.&lt;/p&gt;

&lt;h3 id=&quot;what-do-interpretable-models-look-like&quot;&gt;What do interpretable models look like?&lt;/h3&gt;

&lt;p&gt;There are two broad categories of ways to interpret models: transparency and post-hoc explanations.&lt;/p&gt;

&lt;h5 id=&quot;transparency&quot;&gt;Transparency&lt;/h5&gt;

&lt;p&gt;This is basically “how does the model work?”. For example, simpler models such as linear regression are considered more interpretable than a deep neural network (which is sometimes referred to as a blackbox). Let’s imagine we want to train a model to predict house prices. We could have some features about the house such as the number of bedrooms, square footage, distance to a city with more than 500 000 people etc. A linear model is just taking these features and weights and adding them up.&lt;/p&gt;

&lt;p&gt;$ \text{house price} = w_1 \cdot (\text{number of bedrooms}) + w_2\cdot(\text{square footage}) + w_3\cdot(\text{distance to city}) + … $&lt;/p&gt;

&lt;p&gt;In these models a higher weight means the feature is more important so interpreting which features are more important is really easy. To say the models are interpretable overall assumes that the features themselves are also easily interpretable (e.g. number of bedrooms).&lt;/p&gt;

&lt;p&gt;Other models may not be as easy to interpret directly. For example a random forest uses a bunch of decision trees (think flow charts). While the model may be harder to interpret overall, interpreting an individual decision tree is relatively straightforward.&lt;/p&gt;

&lt;p&gt;Another reason that linear models are typically more interpretable than neural networks/deep learning is that we understand how they work in much more detail. For example, we can prove that a linear model will give a unique solution which is not the case with deep learning models.
However, neural networks typically perform much better than other models. In order to get comparable performance out of these other models there may be tricks needed (e.g. feature engineering) which could make the model less interpretable overall.&lt;/p&gt;

&lt;h5 id=&quot;post-hoc-interpretability&quot;&gt;Post-hoc interpretability&lt;/h5&gt;

&lt;p&gt;Even if we don’t directly know how the model works, we can still get useful information by interpreting the predictions. It’s worth noting that this is how humans explain decisions. We don’t always know the exact cause of a decision but can try to provide a rational explanation after the fact. Interpreting the model after the fact means that we can potentially use models with higher performance (e.g. deep learning models).&lt;/p&gt;

&lt;p&gt;One method of post-hoc interpretability is to have an “explanation by example”. This means getting the model to predict which other examples the model thinks are most similar. In the case of predicting malware or not, the examples most similar to a malicious PDF may be other malicious PDF documents. In the case of deep learning models, we can also gain insight by visualizing the &lt;a href=&quot;/blog/d-is-for-deep-learning&quot;&gt;hidden layers&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Another technique that people use is to train two separate models. The first model makes the predictions, and the second model generates text that explains the prediction. For example, the first model may predict something about an image, while the second model generates a caption for that image.&lt;/p&gt;

&lt;p&gt;Researchers may also train two separate models where one model is more interpretable than the other. For example they may train a deep neural network (with high performance but low interpretability) as well as a random forest model. While the random forest model may not perform as well, it can also give some insight into the problem space. However, this could be potentially misleading as the explanations given by the simpler model may not correspond to why the more complex model works.&lt;/p&gt;

&lt;h3 id=&quot;things-to-keep-in-mind&quot;&gt;Things to keep in mind&lt;/h3&gt;

&lt;p&gt;Interpretability and having explainable models is still a highly active area of research. There are a few things to keep in mind when doing this in practice&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Rules of thumb such as “linear models are more interpretable than neural nets” are generally true (but not always!)&lt;/li&gt;
  &lt;li&gt;When talking about interpretability it is important to clarify what you mean exactly. I highly recommend reading Zachary Lipton’s paper for more specific definitions of the concepts above.&lt;/li&gt;
  &lt;li&gt;There may be cases where an ML model can perform significantly better than humans. Making sure the model is transparent (and potentially reducing performance) is not always the correct decision depending on the overall goal. It might be sufficient to have a better performing model with a human appeals process in place.&lt;/li&gt;
  &lt;li&gt;I recommend using existing libraries such as &lt;a href=&quot;https://github.com/interpretml/interpret&quot;&gt;interpret-ml&lt;/a&gt; which have a wide variety of methods and examples when trying to do this in practice.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;other-resources&quot;&gt;Other resources&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1606.03490.pdf&quot;&gt;Zachary Lipton’s paper&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/interpretml/interpret&quot;&gt;interpret-ml package&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ethics.fast.ai/&quot;&gt;Practical data ethics course from fast.ai&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://lineardigressions.com/episodes/2018/5/13/shap-shapley-values-in-machine-learning&quot;&gt;Podcast on using Shapley values to interpret models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Machine learning models are increasingly used to make decisions in everyday life, and as we’ve discussed before, they can be far from perfect. In addition to being able to appeal the decisions made by a model, it is critical to be able to interpret their predictions. Doing this in practice opens up a huge number of questions including</summary></entry><entry><title type="html">H is for HDBSCAN</title><link href="https://abcsofdatascience.ca/blog/h-is-for-hdbscan" rel="alternate" type="text/html" title="H is for HDBSCAN" /><published>2020-07-01T00:00:00-05:00</published><updated>2020-07-01T00:00:00-05:00</updated><id>https://abcsofdatascience.ca/blog/h-is-for-hdbscan</id><content type="html" xml:base="https://abcsofdatascience.ca/blog/h-is-for-hdbscan">&lt;p&gt;There are many data science problems where you don’t have labelled data and need to use &lt;a href=&quot;/blog/c-is-for-clustering&quot;&gt;clustering&lt;/a&gt; to find related points. For these clustering problems, HDBSCAN is a great algorithm. It was originally created by &lt;a href=&quot;https://link.springer.com/chapter/10.1007/978-3-642-37456-2_14&quot;&gt;Campello et al.&lt;/a&gt; and there is a &lt;a href=&quot;https://github.com/scikit-learn-contrib/hdbscan&quot;&gt;fast Python implementation&lt;/a&gt; written by &lt;a href=&quot;https://twitter.com/leland_mcinnes?lang=en&quot;&gt;Leland Mcinnes&lt;/a&gt; and &lt;a href=&quot;https://github.com/jc-healy&quot;&gt;John Healy&lt;/a&gt;. When I refer to HDBSCAN I’ll be talking about the python implementation/package. It is generally the first clustering method I try for a variety of reasons:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;You don’t need to specify the number of clusters. Other clustering methods such as k-means require that you specify the number of clusters to find in your data, and this is hard to know ahead of time. HDBSCAN will find the natural number of clusters in your data. All you need to specify is the minimum number of points that a cluster should have (which is much easier to have an intuition for).&lt;/li&gt;
  &lt;li&gt;Many other clustering algorithms make assumptions about the shape of the clusters (e.g. they must fit in a circle) or they are all the same density. In real data this is generally not true and HDBSCAN finds clusters with varying shapes/densities.&lt;/li&gt;
  &lt;li&gt;HDBSCAN will label points as noise/outliers. Many clustering algorithms force every point into a cluster. However, real world data is messy and having outliers improves the quality of the clusters (since they aren’t polluted by noise).&lt;/li&gt;
  &lt;li&gt;It generally &lt;em&gt;just works&lt;/em&gt;. I find I spend much less time fiddling with parameters and spend more time looking at my actual data.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Much of this blog is based on examples in the wonderful &lt;a href=&quot;https://hdbscan.readthedocs.io/en/latest/&quot;&gt;documentation for HDBSCAN&lt;/a&gt;. In particular, if you want to see how HDBSCAN compares to other clustering algorithms read &lt;a href=&quot;https://hdbscan.readthedocs.io/en/latest/comparing_clustering_algorithms.html&quot;&gt;this page&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;lets-look-at-an-example&quot;&gt;Let’s look at an example&lt;/h3&gt;

&lt;p&gt;The first thing we need is an &lt;a href=&quot;/blog/e-is-for-embeddings&quot;&gt;embedding&lt;/a&gt;, which as you might recall is just a numeric representation of your data with a way to measure distance between points.
Shown below is a plot of a &lt;a href=&quot;https://github.com/scikit-learn-contrib/hdbscan/blob/master/notebooks/clusterable_data.npy&quot;&gt;sample dataset&lt;/a&gt;. While it is an artificial dataset, it has many properties of real data:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;There are a lot of noisy/outlying points which don’t belong in any cluster&lt;/li&gt;
  &lt;li&gt;The groups of points are different shapes and you can see in some clusters that the points are much closer together, while in others they are less dense.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/images/h_is_for_hdbscan/sample_data.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let’s try clustering this data. First we import HDBSCAN and load our data&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;hdbscan&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'clusterable_data.npy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Clustering the data is as simple as&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;clusterer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hdbscan&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;HDBSCAN&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;min_cluster_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metric&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'euclidean'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;clusterer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit_predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here we are saying that there must be &lt;em&gt;at least&lt;/em&gt; 15 points close together before we say that something is a cluster. How do we measure “close together”? We also specified a Euclidean distance metric. This is the default metric but HDBSCAN can use many &lt;a href=&quot;https://hdbscan.readthedocs.io/en/latest/basic_hdbscan.html#what-about-different-metrics&quot;&gt;other metrics&lt;/a&gt;. Euclidean is the default metric, but it is always better to explicitly state your distance measure for other people reading the code. If we wanted to use cosine distance instead of Euclidean (despite Euclidean being the better choice in this case) we could do&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;clusterer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hdbscan&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;HDBSCAN&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;min_cluster_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metric&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'cosine'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;clusterer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit_predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can see which cluster each point belongs to using&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clusterer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels_&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# [ 5  5  5 ... -1 -1  5]
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The first points shown are part of cluster 5. Points that are outliers are given a label of “-1” so they are easy to filter out. Let’s remake the plot above but colour the points based on their cluster label. The outlying points (part of the -1 cluster) will be grey.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/h_is_for_hdbscan/sample_data_clustered.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can see that the resulting clusters are pretty good. More importantly, they match what we would intuitively pick as the clusters if we had to draw lines around the groups of points.&lt;/p&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;HDBSCAN and its python implementation is a fast clustering algorithm that is easy to use. It naturally handles a lot of the messiness of real world data and lets you spend more time focussing on the problem you are trying to solve. If you want to learn more about how HDBSCAN works and see other examples check out the resources below.&lt;/p&gt;

&lt;h3 id=&quot;other-resources&quot;&gt;Other resources&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/understanding-hdbscan-and-density-based-clustering-121dbee1320e&quot;&gt;Blog on understanding HDBSCAN&lt;/a&gt; which is similar to this blog but goes into much more detail&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html&quot;&gt;How HDBSCAN works&lt;/a&gt; from the official documentation&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://dev.tube/video/dGsxd67IFiU&quot;&gt;HDBSCAN, Fast Density Based Clustering, the How and the Why&lt;/a&gt; - John Healy&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">There are many data science problems where you don’t have labelled data and need to use clustering to find related points. For these clustering problems, HDBSCAN is a great algorithm. It was originally created by Campello et al. and there is a fast Python implementation written by Leland Mcinnes and John Healy. When I refer to HDBSCAN I’ll be talking about the python implementation/package. It is generally the first clustering method I try for a variety of reasons:</summary></entry><entry><title type="html">G is for Gradient Descent</title><link href="https://abcsofdatascience.ca/blog/g-is-for-gradient-descent" rel="alternate" type="text/html" title="G is for Gradient Descent" /><published>2020-05-30T00:00:00-05:00</published><updated>2020-05-30T00:00:00-05:00</updated><id>https://abcsofdatascience.ca/blog/g-is-for-gradient-descent</id><content type="html" xml:base="https://abcsofdatascience.ca/blog/g-is-for-gradient-descent">&lt;p&gt;As I’ve said many times before AI/machine learning/deep learning is &lt;a href=&quot;https://abcsofdatascience.ca/blog/a-is-for-ai&quot;&gt;not magic&lt;/a&gt;. In the case of supervised learning models (including &lt;a href=&quot;https://abcsofdatascience.ca/blog/d-is-for-deep-learning&quot;&gt;deep learning&lt;/a&gt;) you have four things:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Labelled data&lt;/li&gt;
  &lt;li&gt;Features (e.g. image pixels or text)&lt;/li&gt;
  &lt;li&gt;A weight for each feature (since some features are more important than others)&lt;/li&gt;
  &lt;li&gt;An objective (or cost) function which measures how well/poorly your predictions match the labels.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Before we start training a model, we have our data set and choose the features we want to use, as well as an objective function. I’ve mentioned some common objective functions in previous blog posts including cross-entropy loss and root mean squared error (RMSE). An objective function is a function of both the features as well as the weights. Once we’ve chosen the features and objective function, they are fixed while we actually train the model. This means the only thing we can change is the weight of each feature. When we refer to training a model, what we typically mean is finding which values of weights &lt;strong&gt;minimize or maximize&lt;/strong&gt; the objective function. How do we actually find this set of weights? You can imagine trying a bunch of different sets of weights and seeing which gives the best model performance. However, as you might expect there are better ways to find the best set of weights. The broad category of algorithms that find the minimum/maximum values of functions are called &lt;strong&gt;optimization methods&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;gradient-descent&quot;&gt;Gradient descent&lt;/h3&gt;

&lt;p&gt;Gradient descent (and related variants) is a popular optimization technique and it is widely used in a variety of applications, including basically all deep learning models. It is an &lt;strong&gt;iterative&lt;/strong&gt; method, which means it keeps repeating the same steps until some criteria is reached. This stopping criteria is also referred to as &lt;strong&gt;convergence criteria&lt;/strong&gt;. This stopping condition could be “stop when the value of the loss function doesn’t change from step to step”. Let’s imagine we have a simple loss function like the one shown below and our starting position is shown in red. This starting position is typically random, since our weights are randomly initialized. We want to figure out how to get to the bottom of this bowl shaped curve, since this is the set of parameters where our loss function is at the smallest value. As we iterate through this process, we are “learning” better sets of parameters (or weights).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/g_is_for_gradient_descent/loss_function.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Imagine that you are on a hill at this red dot, and need to get to the bottom of the valley. However, you are also blindfolded (which sounds like the world’s worst escape room) so you can’t see where the valley is. You also want to get to the bottom of the hill as quickly as possible, so you want to take as few steps as possible. How would you do this? You would probably try to find the direction where the hill is the steepest and take a step in that direction. You would keep repeating this process until you got to the bottom of the hill. This is exactly what gradient descent is doing. We can calculate the direction with the steepest slope by calculating the derivative (or gradient) of the loss function. We then take a step in that direction, then keep repeating the process until we reach some stopping criteria.&lt;/p&gt;

&lt;p&gt;The key parameter in gradient descent is called the &lt;strong&gt;step size&lt;/strong&gt; or &lt;strong&gt;learning rate&lt;/strong&gt; which says how far to step in the direction of the gradient. It is really important to choose this value correctly. If we choose an appropriate value (like the one shown below) we can take a reasonable number of steps (in this case 7) to get to the bottom of the hill.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/g_is_for_gradient_descent/gradient_descent_optimal.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;However, if we make the learning rate too small, then we need to take a lot of steps and this means that it takes much longer to get to the bottom of the hill.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/g_is_for_gradient_descent/gradient_descent_small_step_size.png&quot; alt=&quot;&quot; title=&quot;Here the learning rate is too small&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If we make it larger, then we can take fewer steps. But this also runs the risk of overshooting the minimum and even risks not converging at all.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/g_is_for_gradient_descent/gradient_descent_too_large.png&quot; alt=&quot;&quot; title=&quot;Here the learning rate is too large&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One way around this problem of having to choose the best learning rate is called “learning rate annealing”. Basically, it means that we start with a large learning rate (so we can quickly take large steps in the right direction). As we continue, we start taking smaller and smaller steps, so that we don’t overshoot the minimum. This has a nice balance between the large and small step sizes which makes choosing an initial learning rate less tricky.&lt;/p&gt;

&lt;h3 id=&quot;local-vs-global-minima&quot;&gt;Local vs global minima&lt;/h3&gt;

&lt;p&gt;Up until now, we have been talking about a very simple loss function which only has one minimum. In practice, loss functions are very messy and have many hills and valleys. Each valley has a &lt;strong&gt;local minima&lt;/strong&gt; and there is one true &lt;strong&gt;global minimum&lt;/strong&gt; which is actually the lowest point. In the loss function shown below, there are two minima. If we start at the red dot and use the method described above, we will get stuck in the valley on the right (a local minimum). However, we would like to get to the bottom of the valley on the left.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/g_is_for_gradient_descent/loss_function_local_minima.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One technique for doing this is called &lt;strong&gt;gradient descent with restarts&lt;/strong&gt;. In this &lt;a href=&quot;https://towardsdatascience.com/https-medium-com-reina-wang-tw-stochastic-gradient-descent-with-restarts-5f511975163&quot;&gt;technique&lt;/a&gt;, you periodically make your learning rate very large (and then slowly make it smaller using annealing). The benefit of this technique is that the large learning rates will help you escape the local minima, and hopefully find the global minimum (or at least a good minimum).&lt;/p&gt;

&lt;h3 id=&quot;gradient-descent-in-practice&quot;&gt;Gradient descent in practice&lt;/h3&gt;

&lt;p&gt;In practice, there are other modifications people apply to gradient descent in order to make it faster/easier to compute.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The form of gradient descent described above is known as “batch gradient descent”. This means that the entire dataset is used to compute the gradients. For large datasets this is impractical since the entire dataset needs to fit in memory. In many applications, especially deep learning, &lt;strong&gt;stochastic gradient descent&lt;/strong&gt; (SGD) is used. Instead of using the entire dataset to calculate the gradient, random subsets are used (called mini-batches). While this is slightly less accurate, it is much faster and more efficient.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Momentum&lt;/strong&gt; is a popular addition to SGD since it makes it faster to compute, and typically gives more accurate results. Here you update the weights using the gradient, but you also use a weighted average of the previous gradients. In our example about finding your way down the hill, you would not stop and try to figure out the exact best slope down the hill for each step. You would continue in roughly the same direction and make minor direction changes as needed. Momentum does a similar thing by including the history of gradients in order to speed things up.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;Gradient descent is a technique used in a wide variety of applications. In particular, it is the workhorse of deep learning, and is what is used when a model “learns” weights. It is a fairly simple idea at its core, and hopefully this gave you an intuition for how it works, as well as some techniques that are used in practice.&lt;/p&gt;

&lt;h3 id=&quot;other-resources&quot;&gt;Other resources&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://wiki.fast.ai/index.php/Gradient_Descent&quot;&gt;An overview of gradient descent with links to more resources&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://youtu.be/5u0jaA3qAGk&quot;&gt;Neural Networks Demystified Part 3: Gradient Descent&lt;/a&gt;. This is a really great short video.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/how-do-we-train-neural-networks-edd985562b73&quot;&gt;How do we ‘train’ neural networks ?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d&quot;&gt;Stochastic Gradient Descent with momentum&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">As I’ve said many times before AI/machine learning/deep learning is not magic. In the case of supervised learning models (including deep learning) you have four things:</summary></entry><entry><title type="html">F is for F1 score</title><link href="https://abcsofdatascience.ca/blog/f-is-for-f1" rel="alternate" type="text/html" title="F is for F1 score" /><published>2020-05-20T00:00:00-05:00</published><updated>2020-05-20T00:00:00-05:00</updated><id>https://abcsofdatascience.ca/blog/f-is-for-f1-score</id><content type="html" xml:base="https://abcsofdatascience.ca/blog/f-is-for-f1">&lt;p&gt;When we are training supervised learning models, we want to measure how well the model is performing. Choosing the correct metric for measuring model performance depends on what kind of task you are doing. There are two main categories of supervised learning tasks&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Classification: Here you are trying to predict which category (or categories) a piece of input data belongs to. For example, given an image you might try to predict if it is a picture of a dog or cat.&lt;/li&gt;
  &lt;li&gt;Regression: Here you are trying to predict a numerical label. For example, you might try to predict the selling price of a house given some features about it, such as neighbourhood, number of bedrooms etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this blog I’ll cover a couple of different methods for measuring model performance. First we’ll focus on classification tasks. To make this more concrete let’s imagine we are training a model to predict if a cookie contains either chocolate chips or raisins.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/f_is_for_f1/cookie_class_small.png&quot; alt=&quot;&quot; title=&quot;Taken from https://commons.wikimedia.org/wiki/File:Chocolate_chip_cookies.jpg and https://commons.wikimedia.org/wiki/File:Raisin_cookie.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-problem-with-accuracy&quot;&gt;The problem with accuracy&lt;/h3&gt;

&lt;p&gt;One way of measuring model performance is called classification accuracy (more commonly referred to as accuracy). This is simply “how many predictions did you get correct out of how many predictions did you make?”. If you correctly predicted the type of cookie 95 times out of 100 predictions, your accuracy would be 95%. However, accuracy only works well if the number of items in each category is roughly equal. If there are many more items in some categories than others, we call this &lt;strong&gt;class imbalance&lt;/strong&gt;. For example, chocolate chip cookies are much more popular than raisin cookies in general. If we assume out of 100 random cookies that 99 of them are chocolate chip, we could get 99% accuracy by guessing chocolate chip every single time. In practice this would be a pretty terrible model, but according to accuracy this is a good model.&lt;/p&gt;

&lt;h3 id=&quot;building-a-confusion-matrix&quot;&gt;Building a confusion matrix&lt;/h3&gt;

&lt;p&gt;If we want to try to address this problem we can try building a &lt;strong&gt;confusion matrix&lt;/strong&gt; (which is less scary/confusing than it sounds). A confusion matrix shows all of the possible combinations of predictions vs the actual labels. We need to pick one category as the “positive class” and the other as the “negative class”. This is arbitrary, so let’s pick chocolate chips as the positive class, and raisin as the negative class. There is a sample confusion matrix shown below.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;Predicted: Chocolate chip&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;Predicted: Raisin&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Actual: Chocolate chip&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;120 (&lt;strong&gt;True Positives&lt;/strong&gt;)&lt;/td&gt;
      &lt;td&gt;2 (&lt;strong&gt;False Negatives&lt;/strong&gt;)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Actual: Raisin&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;10 (&lt;strong&gt;False Positives&lt;/strong&gt;)&lt;/td&gt;
      &lt;td&gt;23 (&lt;strong&gt;True Negatives&lt;/strong&gt;)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;I’m sure we’ve all had the experience of picking up what we thought was a chocolate chip cookie. Upon biting into it we realize “ugh, &lt;em&gt;raisin&lt;/em&gt;”. Don’t get me wrong, I like raisin cookies but it is the mismatch between expectation and reality that is the problem. This mismatch is referred to as a &lt;strong&gt;false positive&lt;/strong&gt; (FP) since we predicted the positive class, but it was actually the negative class. As you would expect, there are also &lt;em&gt;false negatives&lt;/em&gt; (FN) where you predict raisin, but it’s actually chocolate chip. If the prediction matches the actual label, these are referred to as &lt;strong&gt;true positives&lt;/strong&gt; (TP) or &lt;strong&gt;true negatives&lt;/strong&gt; (TN).&lt;/p&gt;

&lt;h3 id=&quot;precision-and-recall&quot;&gt;Precision and recall&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Precision&lt;/strong&gt; and &lt;strong&gt;recall&lt;/strong&gt; are ways of measuring classification quality.&lt;/p&gt;

&lt;h5 id=&quot;precision&quot;&gt;Precision&lt;/h5&gt;

&lt;p&gt;$ \text{Precision} = \frac{TP}{TP + FP}$&lt;/p&gt;

&lt;p&gt;This is the number of true positives, divided by all of the positive results predicted by the model.
In our cookie example, this means “when you predicted chocolate chip, how likely was it to actually be chocolate chip?” If we look at the confusion matrix above, the precision would be&lt;/p&gt;

&lt;p&gt;$ \text{Precision} = \frac{120}{120 + 10} = 0.923$&lt;/p&gt;

&lt;h5 id=&quot;recall&quot;&gt;Recall&lt;/h5&gt;

&lt;p&gt;$ \text{Recall}= \frac{TP}{TP + FN}$&lt;/p&gt;

&lt;p&gt;This is the number of true positives, divided by all of the points that should have been classified as positive. More concretely, this is “out of all of the chocolate chip cookies, how many did you find?”&lt;/p&gt;

&lt;p&gt;$ \text{Recall} = \frac{120}{120 + 2} = 0.984$&lt;/p&gt;

&lt;h3 id=&quot;f1-score&quot;&gt;F1 score&lt;/h3&gt;

&lt;p&gt;A model with high precision but low recall, returns few results but the predictions generally correspond to the actual labels. On the other side, a model with high recall but low precision returns many results, but most of the predictions are incorrect when compared to the labelled data. Obviously, we would like a model with both high precision and high recall. A metric called &lt;strong&gt;F1 score&lt;/strong&gt; combines both precision and recall, and it is a common way to measure model performance&lt;/p&gt;

&lt;p&gt;$F1 = 2\frac{P \cdot R}{P+R}$&lt;/p&gt;

&lt;p&gt;An F1 score of 1.0 corresponds to perfect precision and recall and is close to zero for an extremely bad model. The F1 score is just one way of combining precision and recall, and there are &lt;a href=&quot;https://en.wikipedia.org/wiki/F1_score&quot;&gt;other F measures&lt;/a&gt; which weight precision/recall differently.&lt;/p&gt;

&lt;h3 id=&quot;things-to-consider&quot;&gt;Things to consider&lt;/h3&gt;

&lt;p&gt;In some problems, false positives are more important than false negatives. In others, the opposite is true. Imagine we are trying to predict if a patient has a certain disease or not. A false positive means we think they have the disease, but in actuality they are healthy. Depending on the side effects of treatment, a wrong prediction could have severe consequences. If there are major side effects to treating the disease, we may want to favour precision over recall. On the other hand, there may be cases where treating the disease has minor side effects, and leaving the disease untreated has major consequences. In this case we would want to favour recall, where we find as many instances of the disease as possible. This is obviously a complicated subject, and I highly recommend listening to &lt;a href=&quot;http://lineardigressions.com/episodes/2019/12/22/data-scientists-beware-of-simple-metrics&quot;&gt;this episode of the podcast linear digressions&lt;/a&gt; if you want to know more.&lt;/p&gt;

&lt;h3 id=&quot;regression&quot;&gt;Regression&lt;/h3&gt;

&lt;p&gt;Now we’ll briefly talk about measuring the performance of regression models. Let’s imagine we have a model which predicts the selling price of a house based on it’s square footage. Here we don’t have a small number of categories (e.g. a house could sell for &lt;span&gt;$&lt;/span&gt;503 200 or &lt;span&gt;$&lt;/span&gt;632 777). Below I’ve plotted some fake data where the black points show the predicted price, and the blue points show the actual selling price. One way of measuring the amount of error is called &lt;strong&gt;Mean Absolute Error&lt;/strong&gt; (MAE). This is simply adding up all the differences between the predicted and actual values (shown by the red lines) and dividing by the number of points. The absolute part of MAE just means that you take the absolute value of the differences. If you have one prediction which overestimates the price by &lt;span&gt;$&lt;/span&gt;5000 and another which underestimates by &lt;span&gt;$&lt;/span&gt;5000, the MAE is 10000 (not 0 where the two differences cancel out). A related metric is called &lt;strong&gt;Mean Squared Error&lt;/strong&gt; (MSE) where you square the differences before adding them up. The reason for this that big differences will become even bigger. As an example, a difference of &lt;span&gt;$&lt;/span&gt;2 will become 4 but a difference of &lt;span&gt;$&lt;/span&gt;1000 will become 1000000.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/f_is_for_f1/mean_absolute_error_house_price.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;When working on a supervised learning problem, choosing the correct metric is important. First you should think about if you are working on a classification or regression problem. Then you need to consider which metric best measures what you are trying to achieve. This is just a small summary of some of the ways of measuring model performance. For more info check out the links below or look at the description of cross-entropy loss in &lt;a href=&quot;/blog/d-is-for-deep-learning&quot;&gt;“D is for deep learning”&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;other-resources&quot;&gt;Other resources&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/metrics-to-evaluate-your-machine-learning-algorithm-f10ba6e38234&quot;&gt;Other metrics you can use&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html&quot;&gt;More on precision and recall&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://lineardigressions.com/episodes/2019/12/22/data-scientists-beware-of-simple-metrics&quot;&gt;Beware of simple metrics podcast&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">When we are training supervised learning models, we want to measure how well the model is performing. Choosing the correct metric for measuring model performance depends on what kind of task you are doing. There are two main categories of supervised learning tasks</summary></entry></feed>