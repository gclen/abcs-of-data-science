<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://abcsofdatascience.ca/feed.xml" rel="self" type="application/atom+xml" /><link href="https://abcsofdatascience.ca/" rel="alternate" type="text/html" /><updated>2021-08-02T15:09:25-05:00</updated><id>https://abcsofdatascience.ca/feed.xml</id><title type="html">ABCs of data science</title><subtitle>A gentle introduction to many data science concepts for readers of all backgrounds</subtitle><entry><title type="html">Z is for Zero to Done</title><link href="https://abcsofdatascience.ca/blog/z-is-for-zero-to-done" rel="alternate" type="text/html" title="Z is for Zero to Done" /><published>2021-02-06T00:00:00-06:00</published><updated>2021-02-06T00:00:00-06:00</updated><id>https://abcsofdatascience.ca/blog/z-is-for-zero-to-done</id><content type="html" xml:base="https://abcsofdatascience.ca/blog/z-is-for-zero-to-done">&lt;p&gt;We’ve covered a &lt;a href=&quot;/categories&quot;&gt;lot of topics&lt;/a&gt; in these 26 blogs. Kudos to you if you’ve made it this far! In this blog I want to cover what the end-to-end data science process looks like. A lot of this will depend on the goals of the project and your organization:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Are you trying a smaller data science to learn about a particular topic?&lt;/li&gt;
  &lt;li&gt;Are you trying to build a product to be used by other people?&lt;/li&gt;
  &lt;li&gt;How mature is your organization with regards to data science? If you’re the first one doing this, things will take longer.&lt;/li&gt;
  &lt;li&gt;Do you have a team of people or are you doing this individually?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are 5 major steps you need to do in most data science projects&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Define your problem&lt;/li&gt;
  &lt;li&gt;Prepare your data&lt;/li&gt;
  &lt;li&gt;Train your model&lt;/li&gt;
  &lt;li&gt;Get feedback&lt;/li&gt;
  &lt;li&gt;Productionize your pipeline&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;define-your-problem&quot;&gt;Define your problem&lt;/h3&gt;

&lt;p&gt;As you’re starting you’ll want to answer the following questions. I find it’s helpful to write down your answers or talk about them with a friend/colleague.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;What are you trying to solve?&lt;/strong&gt; Talk about the problem that you are trying to solve. You should make this as &lt;strong&gt;specific&lt;/strong&gt; as possible. If you are trying to do “machine learning for cyber security” figure out what aspect is most interesting/useful. A more specific problem is “can I predict if a file is malware or not?”&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Why are you trying to solve it?&lt;/strong&gt; Describe how solving this problem will benefit people. This could be benefiting yourself (a learning experience), internal clients (make them more productive), or benefit customers/the world more broadly.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;How is this solution going to be used?&lt;/strong&gt; Write down if the problem is solved how your solution will be used. Is it going to make decisions automatically? Is it going to give context to a human so that they have extra information? Is it trying to predict something about an individual data point or is it trying to group related things together?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Who is asking for a solution to this problem?&lt;/strong&gt; Document who is asking for this to be solved. These people could be domain experts, your boss, or someone else. Make sure that everyone has the same expectations. For more about this read &lt;a href=&quot;/blog/y-is-for-you-should-talk-to-your-clients&quot;&gt;Y is for You Should Talk to Your Clients&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;How would you do it manually?&lt;/strong&gt; Try to go through the process of solving the problem manually multiple times. For example, if you’re trying to figure out if a file is malware or not, sit down with some cybersecurity analysts and triage the files together. This will give you a better understanding of the domain and may give you ideas for relevant features.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What tools are you going to use?&lt;/strong&gt; If this is just an individual project you’re free to use whatever you like! If you’re working in a team you’ll need to figure out which tools people have the most experience with. In some cases you might be forced to use an existing set of tools if they are the ones adopted by your organization.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;prepare-your-data&quot;&gt;Prepare your data&lt;/h3&gt;

&lt;p&gt;Without data, it’s pretty hard to do data science! Make sure you have data that you can use (or at least process into something useful).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;What data do you have available?&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Are there external datasets you can use to enrich your data?&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What data processing do you need?&lt;/strong&gt; In all real-world cases you’ll need to do &lt;a href=&quot;/blog/m-is-for-munging-data&quot;&gt;data munging&lt;/a&gt;. For text data you’ll need to apply &lt;a href=&quot;/blog/n-is-for-natural-language-processing&quot;&gt;natural language processing&lt;/a&gt; techniques to create features from it.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Do you need to embed your data or do dimension reduction?&lt;/strong&gt; In many cases you’ll need to embed your data into a feature space. For more information on that I recommend reading &lt;a href=&quot;/blog/e-is-for-embeddings&quot;&gt;E is for Embeddings&lt;/a&gt; and &lt;a href=&quot;/blog/u-is-for-umap&quot;&gt;U is for UMAP&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Do you have labels?&lt;/strong&gt; If you’re working on a supervised learning problem then you need labelled data. For more information about ways to get that read &lt;a href=&quot;/blog/l-is-for-labelling-data&quot;&gt;L is for Labelling Data&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Have you explored your data?&lt;/strong&gt; You’ll need to look at your data and see if there are missing values, &lt;a href=&quot;/blog/v-is-for-visualization&quot;&gt;visualize&lt;/a&gt; it, look for relevant features etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;train-your-model&quot;&gt;Train your model&lt;/h3&gt;

&lt;p&gt;Once you’ve processed your data you’ll actually need to train your model(s).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Does your problem require a supervised or unsupervised model?&lt;/strong&gt; If you are trying to predict something about individual data points you’ll want to use &lt;a href=&quot;/blog/s-is-for-supervised-learning&quot;&gt;supervised learning&lt;/a&gt;. If you’re trying to group stuff together you’ll want to do &lt;a href=&quot;/blog/c-is-for-clustering&quot;&gt;clustering&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What kind of model should you choose?&lt;/strong&gt; This depends on the problem and data type but here are my starting points. If you have images you might want to use &lt;a href=&quot;Jekyll::Drops::SiteDrop/blog/d-is-for-deep-learning&quot;&gt;deep learning&lt;/a&gt; (typically convolutional neural networks are used). For text or tabular data it’s a good idea to start with  &lt;a href=&quot;/blog/x-is-for-xgboost&quot;&gt;Random Forest or Gradient Boosted Trees&lt;/a&gt;. It’s worth trying to get a simple model working (even if it’s not the new shiny thing).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Are there pretrained models you can start from?&lt;/strong&gt; &lt;a href=&quot;/blog/t-is-for-transfer-learning&quot;&gt;Transfer learning&lt;/a&gt; is a powerful technique that can let you make progress quickly.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Do you need to do any post-processing?&lt;/strong&gt; Especially in the case of clustering you might need to apply additional filters on your clusters for them to be useful. For example, your model could find 1000 groups of stuff but a human might only have time to look at 50 groups. Heuristics on top of your clusters can be a useful way to rank your clusters in terms of usefulness.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;get-feedback&quot;&gt;Get feedback&lt;/h3&gt;

&lt;p&gt;You should continuously ask for feedback on all parts of your pipeline. This will help make sure that you and your clients are on the same page.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Is the output of your models useful?&lt;/strong&gt; If you show the results of your model to subject matter experts do they agree with it? You may want to look into &lt;a href=&quot;/blog/i-is-for-interpretability&quot;&gt;model interpretability&lt;/a&gt; methods to try and figure out why your model made a given prediction.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Does your output fit into your customers workflow?&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Are your models biased in any way?&lt;/strong&gt; This can be a hard problem to diagnose but it’s an extremely important one. For more information read &lt;a href=&quot;/blog/b-is-for-bias&quot;&gt;B is for Bias&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;productionize-your-pipeline&quot;&gt;Productionize your pipeline&lt;/h3&gt;

&lt;p&gt;Once you have a successful prototype, it’s time to put it into production. As your doing so, you’ll want to think about the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;How can you access the results of your model?&lt;/strong&gt; One option is setting up a REST API using a framework like &lt;a href=&quot;https://fastapi.tiangolo.com/&quot;&gt;FastAPI&lt;/a&gt;. Many cloud providers like Azure and AWS offer SaaS solutions for this. Are you results available as part of an app?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Are you retraining the model on a regular basis?&lt;/strong&gt; You should retrain the model periodically to ensure that it’s behaving as you expect. You’ll need to measure model drift which is how the performance changes over time on new data.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Is your workflow repeatable?&lt;/strong&gt; For more on this read &lt;a href=&quot;/blog/r-is-for-reproducibility&quot;&gt;R is for Reproducibility&lt;/a&gt;. How easy is it for someone else to maintain your workflow?&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;There are lots of moving parts involved in completing a fully productionized data science pipeline. It’s okay if it seems overwhelming! No one is an expert in everything, which makes it even more important to work with other people. If you have any questions don’t hesitate to leave a comment, send me a message on &lt;a href=&quot;https://twitter.com/abcsofdatasci&quot;&gt;twitter&lt;/a&gt; send me an email at abcsofdatascience [at] gmail [dot] com.&lt;/p&gt;

&lt;h3 id=&quot;other-resources&quot;&gt;Other resources&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://locallyoptimistic.com/post/practical_ml/&quot;&gt;Practical Tips for Real-World Data Science&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://machinelearningmastery.com/process-for-working-through-machine-learning-problems/&quot;&gt;Applied Machine Learning Process&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://machinelearningmastery.com/how-to-define-your-machine-learning-problem/&quot;&gt;How to Define Your Machine Learning Problem&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">We’ve covered a lot of topics in these 26 blogs. Kudos to you if you’ve made it this far! In this blog I want to cover what the end-to-end data science process looks like. A lot of this will depend on the goals of the project and your organization:</summary></entry><entry><title type="html">X is for XGBoost</title><link href="https://abcsofdatascience.ca/blog/x-is-for-xgboost" rel="alternate" type="text/html" title="X is for XGBoost" /><published>2021-02-05T00:00:00-06:00</published><updated>2021-02-05T00:00:00-06:00</updated><id>https://abcsofdatascience.ca/blog/x-is-for-xgboost</id><content type="html" xml:base="https://abcsofdatascience.ca/blog/x-is-for-xgboost">&lt;p&gt;&lt;a href=&quot;https://xgboost.readthedocs.io/en/latest/&quot;&gt;XGBoost&lt;/a&gt; is a software package/algorithm that has been used to train impressive models in recent years (particularly on tabular/structured data). It’s also extremely fast and has wrappers in a wide variety of languages (Python, C++, Ruby, R, etc). For these reasons, as well as the fact that it has been used to win many &lt;a href=&quot;https://www.kaggle.com/&quot;&gt;Kaggle&lt;/a&gt; competitions it has become very popular in the past few years. XGBoost stands for “Extreme Gradient Boosting”, which despite sounding like something on the side of a pre-workout supplement, is a very useful technique for improving ensembles. In this blog I’ll explain how XGBoost and related methods work and the types of problems that it’s applicable for.&lt;/p&gt;

&lt;h3 id=&quot;ensembling&quot;&gt;Ensembling&lt;/h3&gt;

&lt;p&gt;If you remember from &lt;a href=&quot;/blog/s-is-for-supervised-learning&quot;&gt;“S is for Supervised Learning”&lt;/a&gt; an ensemble is a collection of weak learners (such as decision trees). The idea is that each learner on its own is not particularly strong, but in aggregate they will become a stronger learner. There are two main methods to combine the predictions of the weak learners: &lt;strong&gt;averaging&lt;/strong&gt; and &lt;strong&gt;boosting&lt;/strong&gt;.&lt;/p&gt;

&lt;h5 id=&quot;averaging-methods&quot;&gt;Averaging methods&lt;/h5&gt;

&lt;p&gt;In a random forest model, you might have 500 decision trees which are all trained on different subsets of the data as well as different features. Each tree gets a vote based on their prediction and the class with the majority of votes is chosen as the prediction (this is analogous to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Wisdom_of_the_crowd&quot;&gt;wisdom of the crowds&lt;/a&gt;). Let’s imagine you are trying to predict if something is a &lt;a href=&quot;https://www.theincomparable.com/robot/&quot;&gt;Robot or Not&lt;/a&gt;. If 70% of the trees predict “robot”, and 30% of the trees predict “not a robot” then the overall prediction will be “robot”.  This an example of an &lt;strong&gt;averaging ensemble&lt;/strong&gt; because we trained 500 trees independently and then averaged the predictions.&lt;/p&gt;

&lt;h5 id=&quot;boosting-methods&quot;&gt;Boosting methods&lt;/h5&gt;

&lt;p&gt;In &lt;strong&gt;boosting&lt;/strong&gt; we train the individual learners sequentially instead of independently. In theory, the 2nd decision tree can learn from the mistakes of the first, the 3rd tree from the 2nd and so on.  At the very least we hope it will make &lt;em&gt;different&lt;/em&gt; mistakes. &lt;a href=&quot;https://scikit-learn.org/stable/modules/ensemble.html#adaboost&quot;&gt;AdaBoost&lt;/a&gt; (Adaptive Boosting) is a popular method for boosting and is relatively straightforward to understand.&lt;/p&gt;

&lt;p&gt;Let’s imagine we are trying to predict which players should be drafted by the NBA from draft eligible players. We have three features: the players height, average number of points scored per game in their last two seasons, and their teams winning percentage. AdaBoost typically uses &lt;strong&gt;decision stumps&lt;/strong&gt; (as compared to trees) as its weak learner. A decision stump is just a very shallow tree (with only one node and two leaves). This means each stump only looks at one variable (e.g. height) at a time. The decision stump could learn that if a player is taller than 6’6” then they should be drafted (and not drafted if they are shorter than that). Here are the steps of how AdaBoost works:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Train a weak classifier (e.g. our decision stump using height as a variable) on all of the data. Each data point is given a weight (initially they are all equal).&lt;/li&gt;
  &lt;li&gt;Create a decision stump for all variables (height, PPG, average wins). After training check how well each stump performs (did it classify the points correctly?).&lt;/li&gt;
  &lt;li&gt;For all of the data points which were not classified correctly, increase their weight. For correctly classified samples, decrease their weight. This forces the overall model to focus on points that are hard to classify.&lt;/li&gt;
  &lt;li&gt;Repeat steps 2 and 3 until you reach the maximum number of iterations (or all points have been correctly classified).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;At the end, all of the weak learners get a weighted vote to produce the final prediction.&lt;/p&gt;

&lt;h5 id=&quot;gradient-boosted-trees&quot;&gt;Gradient boosted trees&lt;/h5&gt;

&lt;p&gt;If we want to train an ensemble of decision trees how should we do it? We can generalize boosting on decision trees to use arbitrary &lt;a href=&quot;/blog/s-is-for-supervised-learning&quot;&gt;loss functions&lt;/a&gt;. Once we’ve picked a loss function we can use &lt;a href=&quot;/blog/g-is-for-gradient-descent&quot;&gt;gradient descent&lt;/a&gt; to optimize it. Using this technique is typically called &lt;strong&gt;Gradient Boosted Decision Trees&lt;/strong&gt; or &lt;strong&gt;Gradient Boosted Trees&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;when-should-i-use-ensembles-of-decision-trees&quot;&gt;When should I use ensembles of decision trees?&lt;/h3&gt;

&lt;p&gt;Models like Random Forests or Gradient Boosted Trees are a good starting point for many problems.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;They can be used for both classification and regression&lt;/li&gt;
  &lt;li&gt;They work on a wide variety of datasets. In particular, they work well on tabular data (think Excel spreadsheets)&lt;/li&gt;
  &lt;li&gt;Random forests handle missing values and categorical data well&lt;/li&gt;
  &lt;li&gt;They typically handle high dimensional data (large numbers of features) well&lt;/li&gt;
  &lt;li&gt;You can typically &lt;a href=&quot;/blog/i-is-for-interpretability&quot;&gt;interpret&lt;/a&gt; the predictions made by these models&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;XGBoost is a library which implements gradient boosting and is extremely fast. It’s available in a wide variety of languages and frameworks. For Python you can &lt;a href=&quot;https://xgboost.readthedocs.io/en/latest/build.html&quot;&gt;install it&lt;/a&gt; using&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip install xgboost
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Even if you don’t use XGBoost, I recommend starting with decision tree based models as a starting point for most ML problems. You can often get further with some other models but they typically require more hyperparameter optimization and deep understanding of your data.&lt;/p&gt;

&lt;h3 id=&quot;other-resources&quot;&gt;Other resources&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://lineardigressions.com/episodes/2017/1/22/ensemble-algorithms&quot;&gt;Episode of Linear Digressions on Ensembles&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://scikit-learn.org/stable/modules/ensemble.html&quot;&gt;Scikit-learn documentation on ensemble methods&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://xgboost.readthedocs.io/en/latest/index.html&quot;&gt;XGBoost documentation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://xgboost.readthedocs.io/en/latest/tutorials/model.html&quot;&gt;Introduction to boosted trees from XGBoosts documentation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/&quot;&gt;A Gentle Introduction to XGBoost for Applied Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">XGBoost is a software package/algorithm that has been used to train impressive models in recent years (particularly on tabular/structured data). It’s also extremely fast and has wrappers in a wide variety of languages (Python, C++, Ruby, R, etc). For these reasons, as well as the fact that it has been used to win many Kaggle competitions it has become very popular in the past few years. XGBoost stands for “Extreme Gradient Boosting”, which despite sounding like something on the side of a pre-workout supplement, is a very useful technique for improving ensembles. In this blog I’ll explain how XGBoost and related methods work and the types of problems that it’s applicable for.</summary></entry><entry><title type="html">Y is for You Should Talk to Your Clients</title><link href="https://abcsofdatascience.ca/blog/y-is-for-you-should-talk-to-your-clients" rel="alternate" type="text/html" title="Y is for You Should Talk to Your Clients" /><published>2021-02-05T00:00:00-06:00</published><updated>2021-02-05T00:00:00-06:00</updated><id>https://abcsofdatascience.ca/blog/y-is-for-you-should-talk-to-your-clients</id><content type="html" xml:base="https://abcsofdatascience.ca/blog/y-is-for-you-should-talk-to-your-clients">&lt;p&gt;Many of the &lt;a href=&quot;&quot;&gt;previous blogs&lt;/a&gt; in this series have focused on the more technical aspects of data science like training models and measuring performance. In all real world data science projects, you’ll need to do more than just the technical aspects. Unless you’re doing it to learn (a perfectly valid reason!), we typically aren’t doing data science for the sake of doing data science. We don’t want to spend tons of time and effort on a data science project, just to have it dropped on the floor or ignored. It’s crucial to communicate with &lt;strong&gt;everyone&lt;/strong&gt; involved in your data science projects to increase the chances of your projects being adopted/recognized.&lt;/p&gt;

&lt;h3 id=&quot;data-science-is-a-team-sport&quot;&gt;Data science is a team sport&lt;/h3&gt;

&lt;p&gt;Data science involves a wide range of skills and expertise, especially in large projects. You need people who can &lt;a href=&quot;/blog/m-is-for-munging-data&quot;&gt;munge data&lt;/a&gt;, &lt;a href=&quot;/blog/s-is-for-supervised-learning&quot;&gt;train models&lt;/a&gt;, and have subject matter expertise in the specific problem domain. For large projects, you’ll want people with software engineering experience to help you build maintainable code and systems. The traditional Venn diagram for describing data science comes from &lt;a href=&quot;http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram&quot;&gt;Drew Conway&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/y_is_for_you_should_talk_to_your_clients/conway_venn_diagram.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It’s pretty unlikely that one person is an expert in all of these fields (which is why they are referred to as unicorns!). Successful data science projects usually need input from a variety of people. A more accurate Venn diagram might look like&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/y_is_for_you_should_talk_to_your_clients/gartner_venn_diagram.png&quot; alt=&quot;&quot; title=&quot;Taken from https://blogs.gartner.com/christi-eubanks/three-lessons-crossfit-taught-data-science/&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Clearly there is a lot to get right for a successful data science project. In my experience, people tend to focus on the more technical aspects and ignore the customer/client angle. There are a lot more resources for the data preparation and model training parts of this Venn diagram. It’s also important to understand the marketing angle.&lt;/p&gt;

&lt;h3 id=&quot;marketing-your-work&quot;&gt;Marketing your work&lt;/h3&gt;

&lt;p&gt;It’s important to communicate with the people who are going to actually &lt;em&gt;use&lt;/em&gt; the models that you’ve built. People won’t just use your models for no reason, they actually need to be useful. Here are a few things to keep in mind when trying to make sure the project you’re working on is useful. You should talk to your clients (either internal or external) to figure out the problems that they need solving.&lt;/p&gt;

&lt;h5 id=&quot;understand-their-workflow&quot;&gt;Understand their workflow&lt;/h5&gt;

&lt;p&gt;Talk to experts in the problem domain that you’re working in. These could be doctors, business analysts at your company, or just some trusted friends/family. Try to understand what the pain points are in their workflow and figure out ways that you might be able to help solve it. Machine learning works best on specific, well-defined problems. Talking to experts can help you pinpoint these specific problems. Saying I’m going to train a machine learning model to look at MRI images for a specific disease is a more achievable goal than saying “AI/ML for medicine”. You should make sure that everyone is on the same page and using the same language. Additionally, trust the expertise of the people you are talking to! You don’t want to be this person&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://imgs.xkcd.com/comics/here_to_help.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;manage-expectations&quot;&gt;Manage expectations&lt;/h5&gt;

&lt;p&gt;It’s possible you’ve been tasked by your management to “use machine learning”. You should also know by now that machine learning/AI is not magic. For people who are less familiar with the field, it can be hard to tell what is &lt;a href=&quot;https://xkcd.com/1425/&quot;&gt;doable&lt;/a&gt; or still an active area of research. It’s really important to be honest about what is realistically achievable. You should also make it clear if the tools you are developing are intended to take automated actions, or help humans make better decisions.
Fit into their workflow
If possible you should try to avoid building shiny new tools to display your model results. People typically have workflows and existing tools that they are comfortable with. You should try to fit your model results into these tools (or at the very least link your tool from the existing ones).&lt;/p&gt;

&lt;h5 id=&quot;communicate-early-and-often&quot;&gt;Communicate early and often&lt;/h5&gt;
&lt;p&gt;As the project progresses, you should update your clients with any results that you’ve found. You should also ask for their input regularly. For example, if you hear them saying “why did the model make THAT prediction?” you might want to focus on more &lt;a href=&quot;/blog/i-is-for-interpretability&quot;&gt;interpretable&lt;/a&gt; models. It can be very helpful to have a small group of trusted clients who are closely involved with the process. Management or software engineering teams might be familiar with &lt;a href=&quot;https://en.wikipedia.org/wiki/Agile_software_development&quot;&gt;Agile software development&lt;/a&gt; as a way to manage projects. There are some good parts that can be adapted for data science projects. Linear Digressions did a good two part series on Agile for Data Science, talking about &lt;a href=&quot;http://lineardigressions.com/episodes/2018/8/19/agile-development-for-data-scientists-part-1-the-good&quot;&gt;the good&lt;/a&gt; and &lt;a href=&quot;http://lineardigressions.com/episodes/2018/8/26/agile-development-for-data-scientists-part-2-where-modifications-help&quot;&gt;adjustments that should be made&lt;/a&gt;. Be clear in what you are asking for feedback on. If you are looking for feedback on the way you are displaying results (as compared to the results themselves), make that obvious!&lt;/p&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;Data science involves a lot of skills and no one can do it all. We tend to focus on the technical parts of the data science pipeline, but communication and marketing are crucial skills. A person with domain expertise and the ability to communicate with end users and other data scientists is as valuable as the people training the models and preparing the data.&lt;/p&gt;

&lt;h3 id=&quot;other-resources&quot;&gt;Other resources&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://locallyoptimistic.com/post/practical_ml/&quot;&gt;Practical Tips for Real-World Data Science&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/applying-agile-methodology-to-data-science-projects-db50ebbef115#:~:text=The%20Agile%20way%20of%20working%20allows%20data%20scientists%20the%20ability,for%20the%20next%20incremental%20improvement&quot;&gt;Agile for Data Science&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://lineardigressions.com/episodes/2019/12/22/data-scientists-beware-of-simple-metrics&quot;&gt;Data scientists: beware of simple metrics&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://machinelearningmastery.com/how-to-define-your-machine-learning-problem/&quot;&gt;How to define your machine learning problem&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Many of the previous blogs in this series have focused on the more technical aspects of data science like training models and measuring performance. In all real world data science projects, you’ll need to do more than just the technical aspects. Unless you’re doing it to learn (a perfectly valid reason!), we typically aren’t doing data science for the sake of doing data science. We don’t want to spend tons of time and effort on a data science project, just to have it dropped on the floor or ignored. It’s crucial to communicate with everyone involved in your data science projects to increase the chances of your projects being adopted/recognized.</summary></entry><entry><title type="html">W is for Wasserstein GANs</title><link href="https://abcsofdatascience.ca/blog/w-is-for-wasserstein-gans" rel="alternate" type="text/html" title="W is for Wasserstein GANs" /><published>2021-02-04T00:00:00-06:00</published><updated>2021-02-04T00:00:00-06:00</updated><id>https://abcsofdatascience.ca/blog/w-is-for-wasserstein-gans</id><content type="html" xml:base="https://abcsofdatascience.ca/blog/w-is-for-wasserstein-gans">&lt;p&gt;You may have seen or heard of the term “&lt;a href=&quot;https://en.wikipedia.org/wiki/Deepfake&quot;&gt;deepfakes&lt;/a&gt;” where realistic looking images/videos/audio have been generated to look or sound like a specific person (e.g. a famous politician). Deepfakes is a more colloquial term for the field of &lt;strong&gt;synthetic media&lt;/strong&gt; which is the use of AI to generate images/audio/videos. Faking images is not a new phenomenon (e.g. photoshop) but historically faking video has been challenging. 
One of the main drivers of the recent breakthroughs in synthetic media are the use of Generative adversarial networks (GANs). While there are some concerns about how GANs and related techniques will be used more broadly, there are legitimate purposes for it. For example, &lt;a href=&quot;https://arxiv.org/abs/1612.07828&quot;&gt;Apple used GANs&lt;/a&gt; to generate images of faces where they knew the direction the person was looking. This makes it easier to get training data that can be used for other models.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/w_is_for_wasserstein_gans/AI_generated_face.jpg&quot; alt=&quot;&quot; title=&quot;Image of a synthetically generated face. Taken from https://commons.wikimedia.org/wiki/File:AI_generated_face.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;generative-adversarial-networks&quot;&gt;Generative Adversarial Networks&lt;/h3&gt;

&lt;p&gt;GANs are a technique created by &lt;a href=&quot;https://twitter.com/goodfellow_ian?lang=en&quot;&gt;Ian Goodfellow&lt;/a&gt; which involves two &lt;a href=&quot;/blog/d-is-for-deep-learning&quot;&gt;neural networks&lt;/a&gt; pitted against one another. One neural network (called the generator/creator) tries to create realistic looking data (typically starting from random noise). The other network (the discriminator/investigator) is fed a combination of fake and real data and it tries to predict whether the data is real or fake. This is a zero-sum game, so any gains made by the generator are lost by the discriminator. As training goes on, both models will get better at their tasks and the generated data will be more realistic. To steal an analogy from this &lt;a href=&quot;http://lineardigressions.com/episodes/2019/6/30/deepfakes&quot;&gt;great episode of Linear Digressions&lt;/a&gt; it’s like one person is trying to counterfeit money and the other person needs to determine counterfeit from the real money. Below is a high level view of the architecture of a GAN. Initially the generated data is pretty clearly fake&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/w_is_for_wasserstein_gans/gan_arch_initial_training.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;After training the generated data will become much more realistic&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/w_is_for_wasserstein_gans/gan_arch_final_training.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In a standard GAN, the discriminator outputs a &lt;strong&gt;probability&lt;/strong&gt; that the input is real or fake (i.e. some number between 0 and 1). The loss function is typically based on the &lt;a href=&quot;https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence&quot;&gt;Jensen-Shannon divergence&lt;/a&gt; (JS divergence) which is a way of measuring the similarity of two probability distributions. The generator wants to minimize this loss (i.e. the real and fake distributions should look the same), while the discriminator wants to maximize it. It’s worth noting that after training you have both a model that can generate realistic data as well as a model that can distinguish between real and fake data.&lt;/p&gt;

&lt;h3 id=&quot;wasserstein-gans&quot;&gt;Wasserstein GANs&lt;/h3&gt;

&lt;p&gt;Standard GANs can be hard to train in practice. This is because you need to find some stable equilibrium for both the generator and discriminator. &lt;a href=&quot;https://arxiv.org/abs/1701.07875&quot;&gt;Wasserstein GANs&lt;/a&gt; (also known as WGANs) make a few modifications to the standard GAN which make it better in practice.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Instead of using JS divergence, they use Wasserstein distance (also known as earth-mover distance) to measure the similarity of the two probability distributions. This has some nice theoretical justifications (i.e. the math works out).&lt;/li&gt;
  &lt;li&gt;They call the discriminator a “critic”. Instead of having to output a probability (which is restricted to a number between 0 and 1), WGANs don’t have this constraint. This means that there can be bigger differences between the losses, leading to better training.&lt;/li&gt;
  &lt;li&gt;The critic is updated more often than the generator (e.g. 5x more)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are a few other implementation details but these are the bigger ones. Overall they are easier to train and more stable, producing better results. WGANs are just useful for generating images. They’re used to generate text and other kinds of data such as realistic looking &lt;a href=&quot;https://arxiv.org/abs/1911.06285&quot;&gt;domain names&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;stylegan&quot;&gt;StyleGAN&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1812.04948&quot;&gt;StyleGAN&lt;/a&gt; is another GAN extension which was developed by Nvidia. It’s not directly related to WGANs but has produced impressive looking images of faces. I’m not going to go into how they work here, but there is a &lt;a href=&quot;https://towardsdatascience.com/generating-with-style-the-mechanics-behind-nvidias-highly-realistic-gan-images-b6937237e3c6&quot;&gt;good overview&lt;/a&gt; written by Cody Wild.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/w_is_for_wasserstein_gans/stylegan_results.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;GANs are being used in a wide variety of applications including making art, music, and generating training data for other ML models. GANs and synthetic media can definitely seem scary and there are societal implications to consider in addition to the technical ones. This is a very active field with research into generating more realistic media as well as detecting if an image is fake or not. Tools to detect synthetic media will become increasingly important but education can play a large part in making the public more critical in the images/videos they see. While generated outputs are always improving, there are subtle signs to look for to tell if an image is fake or not. This includes things like asymmetry (e.g. someone wearing one earring), noise in the background, or other oddities. &lt;a href=&quot;https://kcimc.medium.com/how-to-recognize-fake-ai-generated-images-4d1f6f9a2842&quot;&gt;Here&lt;/a&gt; is a good article detailing the things to look for when trying to tell if an image is fake.&lt;/p&gt;

&lt;h3 id=&quot;other-resources&quot;&gt;Other resources&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://machinelearningmastery.com/how-to-implement-wasserstein-loss-for-generative-adversarial-networks/&quot;&gt;How to Implement Wasserstein Loss for Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1701.07875&quot;&gt;Wasserstein GAN paper&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://lineardigressions.com/episodes/2016/5/28/neural-nets-play-cops-and-robbers-aka-generative-adversarial-networks&quot;&gt;Episode of Linear Digressions on GANs&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://lineardigressions.com/episodes/2019/6/30/deepfakes&quot;&gt;Episode of Linear Digressions on deepfakes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">You may have seen or heard of the term “deepfakes” where realistic looking images/videos/audio have been generated to look or sound like a specific person (e.g. a famous politician). Deepfakes is a more colloquial term for the field of synthetic media which is the use of AI to generate images/audio/videos. Faking images is not a new phenomenon (e.g. photoshop) but historically faking video has been challenging. One of the main drivers of the recent breakthroughs in synthetic media are the use of Generative adversarial networks (GANs). While there are some concerns about how GANs and related techniques will be used more broadly, there are legitimate purposes for it. For example, Apple used GANs to generate images of faces where they knew the direction the person was looking. This makes it easier to get training data that can be used for other models.</summary></entry><entry><title type="html">V is for Visualization</title><link href="https://abcsofdatascience.ca/blog/v-is-for-visualization" rel="alternate" type="text/html" title="V is for Visualization" /><published>2021-02-03T00:00:00-06:00</published><updated>2021-02-03T00:00:00-06:00</updated><id>https://abcsofdatascience.ca/blog/v-is-for-visualization</id><content type="html" xml:base="https://abcsofdatascience.ca/blog/v-is-for-visualization">&lt;p&gt;One of the most important aspects of data science is being able to effectively communicate and use your data to tell a story. Often you’ve spent a lot of time and effort cleaning your data, training models, and exploring your data.  Visualizing your data is a crucial aspect of being able to tell that story and show off any insights you might have gained. You should always keep the following questions in mind:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Who is your audience?&lt;/strong&gt; Are they other data scientists? Subject matter experts? Other stakeholders (e.g. executives)?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What are you trying to convey?&lt;/strong&gt; It’s worth thinking of visualizations as representations of one aspect of your dataset. Try to make sure you are clearly communicating one aspect/concept.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Are your visualizations easy to understand?&lt;/strong&gt;. You’ve probably spent the most amount of time exploring your data. For people who are less familiar, they need to be able to quickly understand what you are showing. You don’t want people to dismiss the rest of your hard work because of some boring or complicated visualizations.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;How do you make an effective visualization? One of the first questions you need to ask is what type of data you have.&lt;/p&gt;

&lt;h3 id=&quot;types-of-data&quot;&gt;Types of data&lt;/h3&gt;

&lt;p&gt;There are two broad categories of data,  quantitative and qualitative, and each have a couple sub categories.&lt;/p&gt;

&lt;h5 id=&quot;quantitative&quot;&gt;Quantitative&lt;/h5&gt;
&lt;p&gt;This is numeric data and represents something like a measurement or a count. There are two subtypes&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Continuous&lt;/strong&gt;: Here we can have an infinite number of values (e.g. 2, 5.983, 8749). Good examples of continuous data include height, temperature, and average selling price. WIth continuous variables you can compute statistics such as mean, median, standard deviation etc.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Discrete&lt;/strong&gt;: These are still numbers but can’t be meaningfully divided into smaller chunks. For example, you can have 1 or 2 children but not 1.5 children. However,  you can still compute statistics such as mean or standard deviation. You could say the average household had 1.54 children.&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;qualitative&quot;&gt;Qualitative&lt;/h5&gt;

&lt;p&gt;Qualitative data is something that can be categorized based on traits/characteristics (as compared to numbers).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Categorical:&lt;/strong&gt; Categorical data is data that fit into a finite number of categories that have no inherent ordering. Colours (blue, red, green) are classified as categorical data because one colour is not inherently better than another.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Ordinal:&lt;/strong&gt; Ordinal data is a type of categorical data with at least three categories. As you might guess from the name there is an inherent order. A good example of ordinal data is agreement data (strongly disagree to strongly agree) commonly found in surveys.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here are some general rules of thumb for plotting different data types:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;If you have categorical data use a bar chart (especially if you need precise comparisons!). For example, here is a plot comparing the most frequently used tools on the &lt;a href=&quot;https://www.reddit.com/r/dataisbeautiful/&quot;&gt;DataIsBeautiful subreddit&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/images/v_is_for_visualization/DIB-tools-used-bar.png&quot; alt=&quot;&quot; title=&quot;Taken from http://www.randalolson.com/2016/03/11/what-data-visualization-tools-do-rdataisbeautiful-oc-creators-use/&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Use a scatterplot or line plot if you’re comparing two quantitative variables. Here’s a plot comparing NBA/NCAA shooting accuracy vs distance from the basket.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/images/v_is_for_visualization/fgp_by_distance.png&quot; alt=&quot;&quot; title=&quot;Taken from https://toddwschneider.com/posts/nba-vs-ncaa-basketball-shooting-performance/&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;If you have qualitative data (or multiple comparisons in quantitative data) add extra attributes to visually distinguish between data types. You should also not just rely on colour (as some people have trouble distinguishing between colours). For example, texture (making your points dotted or striped) is very effective for categorical data as it provides a visual marker to differentiate between your categories. But this is not effective for quantitative (i.e. is striped bigger than dotted?). Keep these attributes in mind the next time you are making a plot, and think about what is the best way to represent your underlying data.&lt;/li&gt;
  &lt;li&gt;If you need to communicate uncertainty in measurements, you will need to add error bars&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Of course there are many more chart types than histograms, bar charts, scatter plots, and line charts. It’s important to consider the type of data you are plotting as well as your audience. For example, a &lt;a href=&quot;https://xkcd.com/1967/&quot;&gt;violin plot&lt;/a&gt; can be enlightening for other data scientists but might not be as intuitive for other people.&lt;/p&gt;

&lt;h3 id=&quot;plotting-libraries&quot;&gt;Plotting libraries&lt;/h3&gt;

&lt;p&gt;There are loads of plotting libraries (as shown in the graph above) but here are a few of my favourites (mostly Python libraries):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://matplotlib.org/&quot;&gt;Matplotlib&lt;/a&gt; is extremely powerful but there can be a pretty steep learning curve. Fortunately there are many libraries that wrap matplotlib but still let you dive into it when needed. &lt;a href=&quot;/blog/p-is-for-pandas&quot;&gt;Pandas&lt;/a&gt; plotting is one such wrapper.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://seaborn.pydata.org/&quot;&gt;Seaborn&lt;/a&gt; also wraps matplotlib and has a lot of great functionality. For example, &lt;a href=&quot;https://seaborn.pydata.org/generated/seaborn.pairplot.html&quot;&gt;pairplot&lt;/a&gt; will create a plot for every pair of variables in the dataframe you pass it.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ggplot2.tidyverse.org/&quot;&gt;ggplot2&lt;/a&gt; is an R library which is very powerful and can make very nice looking plots.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.bokeh.org/en/latest/index.html&quot;&gt;Bokeh&lt;/a&gt; lets you make interactive plots in Python and is particularly handy when using Jupyter notebooks&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;things-to-consider&quot;&gt;Things to consider&lt;/h3&gt;

&lt;p&gt;Once you’ve decided on what concept you want to convey and picked the appropriate chart type, there are a few other things to consider.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Check that your colour scheme works for both your data and the platform you’re presenting it on. For example, if you’re presenting categorical data, don’t use 7 shades of blue. Your chart might look great on your 5K display at your desk but could look washed out and hard to decipher on the projector screen when you’re giving a presentation.&lt;/li&gt;
  &lt;li&gt;Ensure your axes make sense. Your axes don’t always need to start from zero. In fact you should zoom in if you want to show data fluctuation more precisely. Just don’t make your graph look like this&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/v_is_for_visualization/eu_payment.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Be careful about using logarithmic axes. For the right audience (who are used to interpreting them) they can be okay but you need to make it clear that it’s a nonlinear scale. You can try normalizing your points to the mean instead.&lt;/li&gt;
  &lt;li&gt;Avoid overplotting. Overplotting is when data or labels in a visualization overlap so it can be hard to distinguish individual points. Try making your points semi-transparent or hollow circles so that it makes it easier to see specific points.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/v_is_for_visualization/overplotting.png&quot; alt=&quot;&quot; title=&quot;Taken from https://python-graph-gallery.com/wp-content/uploads/134_Fighting_overplotting1.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;Effective visualizations can help you deliver a story about your data/models and any insights that you’ve found. In addition to convincing stakeholders/clients, they can be extremely helpful in exploratory data analysis to convince yourself about your understanding of the data. There are many things to consider (and it’s easy to make misleading charts) when creating a visualization but it’s worth getting it right.&lt;/p&gt;

&lt;h3 id=&quot;other-resources&quot;&gt;Other resources&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/data-visualization-101-7-steps-for-effective-visualizations-491a17d974de&quot;&gt;Data Visualization 101: 7 Steps for Effective Visualizations&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ilovecharts.tumblr.com/&quot;&gt;ilovecharts&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://statisticsbyjim.com/basics/data-types/&quot;&gt;Guide to Data Types and How to Graph Them in Statistics&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.tylervigen.com/spurious-correlations&quot;&gt;Spurious correlations&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://heap.io/blog/data-stories/how-to-lie-with-data-visualization&quot;&gt;How to Lie with Data Visualization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">One of the most important aspects of data science is being able to effectively communicate and use your data to tell a story. Often you’ve spent a lot of time and effort cleaning your data, training models, and exploring your data. Visualizing your data is a crucial aspect of being able to tell that story and show off any insights you might have gained. You should always keep the following questions in mind:</summary></entry><entry><title type="html">U is for UMAP</title><link href="https://abcsofdatascience.ca/blog/u-is-for-umap" rel="alternate" type="text/html" title="U is for UMAP" /><published>2021-02-01T00:00:00-06:00</published><updated>2021-02-01T00:00:00-06:00</updated><id>https://abcsofdatascience.ca/blog/u-is-for-umap</id><content type="html" xml:base="https://abcsofdatascience.ca/blog/u-is-for-umap">&lt;p&gt;Often in data science we have data with multiple dimensions/features that we want to visualize or &lt;a href=&quot;/blog/e-is-for-embeddings&quot;&gt;embed&lt;/a&gt; for further analysis. UMAP (Uniform Manifold Approximation and Projection) is one method for doing dimension reduction which will help us with visualizations/embeddings. What do I mean by dimensions? You can think of a dimension as a column in a table or a spreadsheet. For example, let’s say we have a survey about people’s movie preferences. Each person was asked to rate a genre of movies on a scale of 0-5. By looking at the table, we can see that Alice and Mallory have similar tastes. Bob and Trent also seem to like the same genres as well. Since there are 5 different columns/movie genres we say that this data has 5 &lt;strong&gt;dimensions&lt;/strong&gt;.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;Comedy&lt;/th&gt;
      &lt;th&gt;Sci-Fi&lt;/th&gt;
      &lt;th&gt;Drama&lt;/th&gt;
      &lt;th&gt;Horror&lt;/th&gt;
      &lt;th&gt;Action&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Alice&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Bob&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Mallory&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Trent&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Wendy&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;To get a better understanding of this data we want to visualize it. Unfortunately, unless you live in the &lt;a href=&quot;https://www.imdb.com/title/tt0816692/&quot;&gt;Interstellar universe&lt;/a&gt;, humans can only see in 3 dimensions or less. We want to find a way to represent this data in 2 dimensions where points that are close in the higher dimensional space are close in the lower dimensional space. More concretely, in the new 2D representation, Alice should be close to Mallory and Bob should be close to Trent (because they are similar in the higher dimension). &lt;strong&gt;Dimension reduction&lt;/strong&gt; is a general term for any method that represents high-dimensional data into a lower dimensional space. It’s also known as &lt;strong&gt;embedding&lt;/strong&gt; your data into a lower dimensional space (and I’ll use the terms interchangeably). The two new dimensions of the embedding are related to the original features but are not just a subset. In our movie example it is not just as simple as saying “Comedy and Horror are the important features”. How you combine the original features into a new representation depends on the method of dimension reduction used.&lt;/p&gt;

&lt;h3 id=&quot;methods-for-dimension-reduction&quot;&gt;Methods for dimension reduction&lt;/h3&gt;

&lt;p&gt;There are &lt;a href=&quot;https://www.youtube.com/watch?v=9iol3Lk6kyU&quot;&gt;many methods&lt;/a&gt; for doing dimension reduction but here are three popular ones:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Principal Component Analysis (PCA). This is fast, well understood, and the resulting dimensions are &lt;em&gt;kind of&lt;/em&gt; interpretable.&lt;/li&gt;
  &lt;li&gt;t-distributed stochastic neighbor embedding (t-SNE). Mostly used for visualization and is used in a wide variety of applications.&lt;/li&gt;
  &lt;li&gt;Uniform Manifold Approximation and Projection (UMAP). Developed by &lt;a href=&quot;https://twitter.com/leland_mcinnes?lang=en&quot;&gt;Leland McInnes&lt;/a&gt; and &lt;a href=&quot;https://github.com/jc-healy&quot;&gt;John Healy&lt;/a&gt; (who also made &lt;a href=&quot;/blog/h-is-for-hdbscan&quot;&gt;HDBSCAN&lt;/a&gt;) it has been growing in popularity in recent years. It’s faster than t-SNE and arguably preserves the higher dimensional structure better than t-SNE.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;using-umap-for-document-embeddingclustering&quot;&gt;Using UMAP for document embedding/clustering&lt;/h3&gt;

&lt;p&gt;I’m intending this blog to give you the overall flavour of how dimension reduction methods like UMAP are used. If you’re interested in using UMAP I recommend reading the wonderful &lt;a href=&quot;https://umap-learn.readthedocs.io/en/latest/basic_usage.html&quot;&gt;getting started guide&lt;/a&gt; in the documentation. There’s a &lt;a href=&quot;https://www.youtube.com/watch?v=nq6iPZVUxZU&quot;&gt;great video&lt;/a&gt; from Leland McInnes giving a higher level intuition of how it works. If you’re brave (or have a background in topological data analysis) you can dig into the &lt;a href=&quot;https://arxiv.org/abs/1802.03426&quot;&gt;mathematical details&lt;/a&gt; of why UMAP works.&lt;/p&gt;

&lt;p&gt;As a more concrete example let’s imagine we want to find groups of related forum posts. We can use the &lt;a href=&quot;http://qwone.com/~jason/20Newsgroups/&quot;&gt;20 newsgroups dataset&lt;/a&gt; which is a collection of newsgroup documents across different topics. This example combines a lot of ideas we’ve seen in previous blog posts. If you want code and further explanations to go along with this example, I recommend reading this [document embedding tutorial][https://umap-learn.readthedocs.io/en/latest/document_embedding.html].&lt;/p&gt;

&lt;p&gt;We want to find groups of related things, which sounds like we’ll need to use a &lt;a href=&quot;/blog/c-is-for-clustering&quot;&gt;clustering algorithm&lt;/a&gt;. The clustering algorithm can’t just use the raw data on its own, so we’ll need to pass in an embedding. You might recall from &lt;a href=&quot;/blog/e-is-for-embeddings&quot;&gt;“E is for Embeddings”&lt;/a&gt; that an embedding requires two things:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;A numeric representation of your data (because we need to do math)&lt;/li&gt;
  &lt;li&gt;A distance measure (so we can determine how close/far two points are from one another)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let’s start with the numeric representation aspect. Forum posts are mostly text so we’ll need to use natural language processing (NLP) techniques. To represent a forum post, we can just count how often a word from a vocabulary appears in that post. This is known as &lt;strong&gt;count vectorization&lt;/strong&gt; and there are a lot more details in &lt;a href=&quot;/blog/n-is-for-natural-language-processing&quot;&gt;“N is for NLP”&lt;/a&gt;. This will give us a word-document matrix that could look something like&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;the&lt;/th&gt;
      &lt;th&gt;pizza&lt;/th&gt;
      &lt;th&gt;baseball&lt;/th&gt;
      &lt;th&gt;…&lt;/th&gt;
      &lt;th&gt;CPU&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Post 1&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;…&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Post 2&lt;/td&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;…&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Post 3&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;…&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Each row corresponds to a forum post and each column represents a word in the vocabulary. And the value of each cell in the table is how often that word appeared in a given post. Now we have a numeric representation of our data! Now we just need a way of measuring distance between two posts. For example, it looks like post 1 and post 3 are similar because they are both talking about baseball. The fact that we have a series of counts makes Hellinger distance a good choice. There are more details on Hellinger distance in &lt;a href=&quot;/blog/j-is-for-jaccard&quot;&gt;“J is for Jaccard Metric”&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If we calculate the word-document matrix for the 20 newsgroups dataset we get 34880 unique words over 18846 forum posts! This also ignores all words that occur less than 5 times in the whole corpus. Now as you can imagine this is a &lt;strong&gt;sparse matrix&lt;/strong&gt; (there are a lot of zeroes) because most forum posts don’t have 35000 unique words in them. This means that all of the points are spread far apart. Density based clustering algorithms like &lt;a href=&quot;/blog/h-is-for-hdbscan&quot;&gt;HDBSCAN&lt;/a&gt; assume that clusters are groups of points that are close together. We need to use dimension reduction to go from 34880 dimensions to a lower dimensional space.&lt;/p&gt;

&lt;p&gt;In 34880 dimensions, all the points are spread far apart. However, while 2D is useful for visualization, the points can get smushed together and you can lose information which will help you distinguish groups of posts. Fortunately, UMAP lets you embed into an arbitrary number of dimensions such as 10, 25, or 50. I find it is useful to try clustering on data that has been embedded into 10 or 20 dimensions as you data is no longer sparse, but you still can keep a lot of information about the individual data points. Finding which lower dimensional space works best is mostly a matter of trying different values and seeing what works best. I recommend trying different values like 10, 20, 40 (as compared to 10, 11, 12 etc).&lt;/p&gt;

&lt;p&gt;We can also embed the 20 newsgroups data into 2D for visualization and colour the points by the forum topic (e.g. rec.sport.hockey or comp.sys.ibm.pc.hardware). This was done by count vectorizing the data and using Hellinger distance as described.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/u_is_for_umap/20newsgroups_hellinger_counts.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This embedding looks pretty good. There are groups that you would expect to be together (e.g. the sports related topics). Additionally, the big clump in the middle has topics that are very related (computer hardware). On this embedding (or one in a slightly higher dimension) we can run a clustering algorithm to find all the related posts without having to use the topic labels.&lt;/p&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;UMAP is a great tool to have in your data science toolbox for dimension reduction. This blog barely scratches the surface of what it can do but here are some highlights&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;UMAP works on a wide variety of data types such as images, text, and tabular data. As long as you have a numeric representation of that data and a meaningful distance measure you can embed it with UMAP.&lt;/li&gt;
  &lt;li&gt;The canonical implementation of UMAP is in &lt;a href=&quot;https://github.com/lmcinnes/umap&quot;&gt;Python&lt;/a&gt; but there are implementations in other languages such as &lt;a href=&quot;https://github.com/tkonopka/umap&quot;&gt;R&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;UMAP is actively developed and there are useful features being added all the time. For example, the latest release makes it easier to align embeddings over time which is incredibly powerful.&lt;/li&gt;
  &lt;li&gt;You can also combine dimension reduction methods like PCA and UMAP. For very high dimensional data it is a common workflow to apply PCA first and then UMAP on the embedded data.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;other-resources&quot;&gt;Other resources&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=OtVR_ZnXLu4&amp;amp;list=PLGVZCDnMOq0pHVE3SB0ecki__VMncQPKo&amp;amp;index=41&amp;amp;t=0s&quot;&gt;Embed all the things - John Healy (talk from Pydata Los Angeles 2019)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://umap-learn.readthedocs.io/en/latest/index.html&quot;&gt;UMAP documentation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=nq6iPZVUxZU&quot;&gt;UMAP Uniform Manifold Approximation and Projection for Dimension Reduction - SciPy 2018&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://lineardigressions.com/episodes/2019/12/22/umap-the-latest-in-dimensionality-reduction-for-clustering&quot;&gt;An episode of the podcast Linear Digressions on UMAP&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c&quot;&gt;A One-Stop Shop for Principal Component Analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Often in data science we have data with multiple dimensions/features that we want to visualize or embed for further analysis. UMAP (Uniform Manifold Approximation and Projection) is one method for doing dimension reduction which will help us with visualizations/embeddings. What do I mean by dimensions? You can think of a dimension as a column in a table or a spreadsheet. For example, let’s say we have a survey about people’s movie preferences. Each person was asked to rate a genre of movies on a scale of 0-5. By looking at the table, we can see that Alice and Mallory have similar tastes. Bob and Trent also seem to like the same genres as well. Since there are 5 different columns/movie genres we say that this data has 5 dimensions.</summary></entry><entry><title type="html">T is for Transfer Learning</title><link href="https://abcsofdatascience.ca/blog/t-is-for-transfer-learning" rel="alternate" type="text/html" title="T is for Transfer Learning" /><published>2021-01-31T00:00:00-06:00</published><updated>2021-01-31T00:00:00-06:00</updated><id>https://abcsofdatascience.ca/blog/t-is-for-transfer-learning</id><content type="html" xml:base="https://abcsofdatascience.ca/blog/t-is-for-transfer-learning">&lt;p&gt;When you set out to learn something new, you typically aren’t starting completely from scratch. For example, you probably didn’t have to learn how to read English just to read this blog. &lt;strong&gt;Transfer learning&lt;/strong&gt; is a way to take a model that has already been trained on a related problem and use it as a starting point. This has a few benefits:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Many sophisticated models take a lot of time/money/computational power to train. Being able to start from these models reduces the overall cost of training a new model.&lt;/li&gt;
  &lt;li&gt;Transfer learning typically improves performance (in addition to being easier to train).&lt;/li&gt;
  &lt;li&gt;You typically need less labelled data for your problem with transfer learning. As I’ve blogged about &lt;a href=&quot;/blog/l-is-for-labelling-data&quot;&gt;before&lt;/a&gt; getting labelled data can be challenging.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Transfer learning is most commonly applied to &lt;a href=&quot;/blog/d-is-for-deep-learning&quot;&gt;deep learning&lt;/a&gt; models for computer vision (images/videos) and &lt;a href=&quot;/blog/n-is-for-natural-language-processing&quot;&gt;natural language processing&lt;/a&gt; applications (text).&lt;/p&gt;

&lt;h3 id=&quot;classifying-cats-and-dogs&quot;&gt;Classifying cats and dogs&lt;/h3&gt;

&lt;p&gt;One of the most popular introductory Kaggle competitions is &lt;a href=&quot;https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition&quot;&gt;classifying images of cats and dogs&lt;/a&gt;. Here we need to predict if an image contains a cat or a dog.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/t_is_for_transfer_learning/casper_small.jpg&quot; alt=&quot;&quot; title=&quot;My dog Casper&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Instead of training the model completely from scratch, we can start with a model trained on the &lt;a href=&quot;http://www.image-net.org/&quot;&gt;ImageNet dataset&lt;/a&gt;. This is a dataset with millions of images that all belong to thousands of categories. For example, some images are categorized as “flamingo” or “basketball”. The &lt;a href=&quot;https://en.wikipedia.org/wiki/ImageNet#ImageNet_Challenge&quot;&gt;ImageNet challenge&lt;/a&gt; is a competition to train the best model to predict the correct class out of &lt;a href=&quot;https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a&quot;&gt;1000 preselected classes&lt;/a&gt;. Many of these classes are specific dog breeds (e.g. “Pembroke, Pembroke Welsh corgi”) but we are interested in the more generic question of “dog or cat?”. We can take a model that has already been trained on the ImageNet dataset and &lt;strong&gt;fine-tune&lt;/strong&gt; it to our specific problem. These &lt;strong&gt;pretrained&lt;/strong&gt; models are typically deep neural networks (specifically convolutional neural networks or CNNs).&lt;/p&gt;

&lt;p&gt;As a refresher, let’s look at how a neural network works at a high level. There is an input layer where the raw data is fed. In the case of images this is pixels. There is an output layer which has a neuron for each class. In our case we have two classes: cats and dogs. In the middle there are hidden layers with weights for each feature. In deep neural networks the features come from the previous layer (e.g. the first hidden layer uses the input layer, the second layer uses the first etc).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/t_is_for_transfer_learning/casper_nn.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When we train a model from scratch, these weights are initialized randomly. In transfer learning, we start with the weights from the pretrained model. This means that we need to use the same model architecture (number of hidden layers, how they are connected etc). Popular pre-trained models/architectures for this ImageNet include VGG16, VGG19, ResNet50, Inception V3, and Xception (though you don’t really need to worry about this). We can change the number of neurons in the output layer, as the original model had 1000 neurons in its output layer while we only have two.&lt;/p&gt;

&lt;p&gt;In CNNs the earlier layers (closer to the input layer) typically learn to identify more basic features about the images. For example, the first layer could identify things like horizontal or vertical lines. The second layer could identify corners while later layers can identify much more complicated structures like eyes or paws. The relevant features (horizontal/vertical lines) from the earlier layers probably won’t change much between the pretrained model and our model. The last few layers are much more likely to be different between the pretrained model and our model. For example, features that were relevant to identifying a basketball are not particularly useful for the cats vs dogs problem. Of course, this isn’t true if all of your dog pictures come from the Air Bud movies. One technique that people use when fine-tuning models is &lt;strong&gt;layer freezing&lt;/strong&gt;. This means that you don’t let the weights in the earlier layers change, while the weights in later layers can be updated. A related technique is called &lt;strong&gt;discriminative fine-tuning&lt;/strong&gt; which is where you have different &lt;a href=&quot;/blog/g-is-for-gradient-descent&quot;&gt;learning rates&lt;/a&gt; for each layer.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/t_is_for_transfer_learning/discriminative_fine_tuning.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;transfer-learning-for-nlp&quot;&gt;Transfer learning for NLP&lt;/h3&gt;

&lt;p&gt;Transfer learning has been applied to computer vision for years. In the past couple of years it has also gained a lot of traction in NLP. Sebastien Ruder, one of the authors of &lt;a href=&quot;https://arxiv.org/abs/1801.06146&quot;&gt;ULMFiT&lt;/a&gt; (Universal Language Model Fine-tuning for Text Classification), wrote a blog post titled “&lt;a href=&quot;https://ruder.io/nlp-imagenet/&quot;&gt;NLP’s ImageNet moment has arrived&lt;/a&gt;”. In this post he outlines three methods for doing full transfer learning on text, including ULMFiT, ELMo, and Transformers. Historically, the first layer of deep learning models for NLP were initialized with word embeddings. The rest of the network was initialized randomly and needed to be trained from scratch. Each of these techniques uses a different approach (though they all use pretrained language models), but I’ll talk about how ULMFiT works at a high level.&lt;/p&gt;

&lt;p&gt;Let’s imagine we want to train a classifier to predict if an email is spam/not spam. To give ourselves a headstart we can start with a language model. You might remember from &lt;a href=&quot;/blog/n-is-for-natural-language-processing&quot;&gt;N is for NLP&lt;/a&gt; that a language model is just a model that tries to predict the next word in a body of text. The corpus (set of documents) used to train the language model doesn’t need to be the same as what you are using for your downstream task (spam classification). A common approach is to train a language model on a large corpus such as English wikipedia. Training a model on all of wikipedia takes a huge amount of computational time and resources. Fortunately, someone has already done this and we can fine-tune our language model on the documents that we care about. When we are fine-tuning the language model, we start with a model that has been trained to predict the next word in a Wikipedia article. We then train the model to try to predict the next word in an email. Once we have fine-tuned our language model, we can slightly modify it to do text classification (spam/not spam).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/t_is_for_transfer_learning/ulmfit.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In practice there are a few things to keep in mind:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;When using a pretrained model, you need to perform the exact same &lt;a href=&quot;/blog/m-is-for-munging-data&quot;&gt;preprocessing&lt;/a&gt; that was done for the original model.&lt;/li&gt;
  &lt;li&gt;This method assumes that the way language is used in your corpus (e.g. emails) is similar to the general corpus (e.g. Wikipedia). It might not work as well if you try to predict tweet sentiments using academic papers as the general corpus.&lt;/li&gt;
  &lt;li&gt;One benefit of this approach is that you don’t need extra labelled data for fine-tuning the language model. Let’s say you have 1500 emails labelled as spam/not spam. However, you might have an extra 50 000 emails without labels. Because the language models are self supervised (the word you are trying to predict is the label), you can use this extra unlabelled data to improve the performance of your model.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;Transfer learning is an extremely powerful technique that can improve performance, training time, and reduce the amount of labelled data required. It’s worth keeping in mind that you will inherit any &lt;a href=&quot;/blog/b-is-for-bias&quot;&gt;bias&lt;/a&gt; that exists in the pretrained models (or their underlying datasets). You also need to make sure the dataset used to train the pretrained model is similar to the dataset you are trying to use. Starting with a model trained on images of puppies will not be helpful when trying to classify medical images. Knowing when to use pretrained models comes with experience as well as asking domain experts.&lt;/p&gt;

&lt;h3 id=&quot;other-resources&quot;&gt;Other resources&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=5gCQvuznKn0&amp;amp;list=PLtmWHNX-gukKocXQOkQjuVxglSDYWsSh9&amp;amp;index=10&amp;amp;t=0s&quot;&gt;Transfer learning (fastai NLP video 9)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ruder.io/nlp-imagenet/&quot;&gt;NLP’s ImageNet moment has arrived&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.image-net.org/&quot;&gt;ImageNet&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://machinelearningmastery.com/transfer-learning-for-deep-learning/#:~:text=Transfer%20learning%20is%20a%20machine,model%20on%20a%20second%20task.&quot;&gt;A Gentle Introduction to Transfer Learning for Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">When you set out to learn something new, you typically aren’t starting completely from scratch. For example, you probably didn’t have to learn how to read English just to read this blog. Transfer learning is a way to take a model that has already been trained on a related problem and use it as a starting point. This has a few benefits:</summary></entry><entry><title type="html">S is for Supervised Learning</title><link href="https://abcsofdatascience.ca/blog/s-is-for-supervised-learning" rel="alternate" type="text/html" title="S is for Supervised Learning" /><published>2021-01-17T00:00:00-06:00</published><updated>2021-01-17T00:00:00-06:00</updated><id>https://abcsofdatascience.ca/blog/s-is-for-supervised-learning</id><content type="html" xml:base="https://abcsofdatascience.ca/blog/s-is-for-supervised-learning">&lt;p&gt;Supervised learning is one of the major categories of machine learning, and it helps us predict something about a given piece of data. Many of the concepts about supervised learning have come up in previous posts, but I’m hoping that this will provide a clearer picture of how they all fit together.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Supervised learning&lt;/strong&gt; means that you have a labeled training set with some set of features and the given output. One category of supervised learning is regression which happens when you have continuous output data. For example, given features like the square footage of a house, predict the selling price. Classification is the other major category of supervised learning which occurs when you have discrete output (i.e a finite number of categories). For example, you are trying to predict if an email is spam
or not. There are only two possible outcomes in this case (spam or not spam). In regression there are an infinite number of possibilities (i.e. the house could sell for &lt;span&gt;$&lt;/span&gt;500 000, &lt;span&gt;$&lt;/span&gt;500 001, &lt;span&gt;$&lt;/span&gt;500 002 etc).&lt;/p&gt;

&lt;h3 id=&quot;how-does-supervised-learning-work&quot;&gt;How does supervised learning work?&lt;/h3&gt;

&lt;p&gt;To train any supervised learning model you need to have four things:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Labelled data&lt;/li&gt;
  &lt;li&gt;Features&lt;/li&gt;
  &lt;li&gt;An objective (or cost) function&lt;/li&gt;
  &lt;li&gt;A weight for each feature&lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&quot;labelled-data&quot;&gt;Labelled data&lt;/h5&gt;

&lt;p&gt;As we’ve discussed before, getting high quality &lt;a href=&quot;/blog/l-is-for-labelling-data&quot;&gt;labelled data&lt;/a&gt; is hard. It’s often extremely time intensive (and also very boring). Depending on the problem it can also require a substantial amount of expertise. For example, anyone can label images as having a dog or a cat but it requires medical professionals to label MRI scans for a particular disease.&lt;/p&gt;

&lt;h5 id=&quot;features&quot;&gt;Features&lt;/h5&gt;

&lt;p&gt;This is the processed data that gets put into your model. &lt;a href=&quot;/blog/m-is-for-munging-data&quot;&gt;Processing your data&lt;/a&gt; and extracting relevant features is typically the bulk of the work involved in training a ML model. In images the raw pixels could be used as features (this is common in deep learning). &lt;a href=&quot;/blog/n-is-for-natural-language-processing&quot;&gt;Natural language processing&lt;/a&gt; techniques are common for extracting features from text data. Getting useful features from tabular (i.e. in a table or CSV) data usually involves some level of expertise about the problem itself. In many cases you can get much better performance from a model by improving the features, as compared to using more sophisticated models.&lt;/p&gt;

&lt;h5 id=&quot;objective-functions&quot;&gt;Objective functions&lt;/h5&gt;

&lt;p&gt;An objective function (also called a cost function or loss function) tells us how well our predictions match the labeled data. It should give us more information than if the prediction was correct/incorrect. If the prediction was wrong, it should also tell us how wrong the prediction was. An example of an objective function for regression problems is Mean Absolute Error (MAE). This is just the difference between the actual value and predicted value. When trying to predict house prices being off by &lt;span&gt;$&lt;/span&gt;300 000 is worse than being off by &lt;span&gt;$&lt;/span&gt;1000. For classification problems, one common objective function is cross-entropy loss. It takes into account if the prediction was correct as well as how confident the model was about the prediction. If you are confident about a correct prediction you will be rewarded, but if you are confident about an incorrect prediction you’ll be penalized heavily. However, if you are not confident about your prediction the reward/penalty will be much lower. There are more examples of objective functions in &lt;a href=&quot;/blog/d-is-for-deep-learning&quot;&gt;D is for Deep Learning&lt;/a&gt; and &lt;a href=&quot;/blog/f-is-for-f1&quot;&gt;F is for F1 Score&lt;/a&gt;.&lt;/p&gt;

&lt;h5 id=&quot;feature-weighting&quot;&gt;Feature weighting&lt;/h5&gt;

&lt;p&gt;Let’s imagine we have an extremely simple model. We are going to try to predict the price of a house given two features: location (the distance to a major city), and if the house has a dishwasher or not. As you might expect, some features (e.g. location) are more important than others (e.g dishwasher). When we start training a model, the input data, features and objective function stay the same. The only thing that changes is the weighting of each feature. Typically these weights are chosen randomly before training. In our house example on the first iteration we might have&lt;/p&gt;

&lt;p&gt;$ \text{Prediction} = (0.33)\cdot(\text{Location}) + (0.67)\cdot(\text{Has dishwasher})$&lt;/p&gt;

&lt;p&gt;Here the dishwasher feature is twice as important as the location. This would probably lead to a bad model. After a few training iterations we might have&lt;/p&gt;

&lt;p&gt;$ \text{Prediction} = (0.99)\cdot(\text{Location}) + (0.01)\cdot(\text{Has dishwasher})$&lt;/p&gt;

&lt;p&gt;This makes more intuitive sense. Obviously the location matters a lot more but all other things being equal, a house with a dishwasher would sell for a bit more.&lt;/p&gt;

&lt;h3 id=&quot;training-a-model&quot;&gt;Training a model&lt;/h3&gt;

&lt;p&gt;Training a model refers to finding the feature weights which minimize/maximize the objective function. For some objective functions we want to find the minimum value, while for others we want to find the maximum value. There are different ways of finding these min/max values (called optimization methods) but &lt;a href=&quot;/blog/g-is-for-gradient-descent&quot;&gt;gradient descent&lt;/a&gt; is an extremely common one.&lt;/p&gt;

&lt;p&gt;When we train a model we want to ensure our model does not overfit/underfit the data. We can do this by splitting our dataset into a training and test set. We only update the model weights based on data in the training set. The test set is used to evaluate the model on data it hasn’t seen. For more details on preventing over/underfitting see &lt;a href=&quot;/blog/k-is-for-kfold-cross-validation&quot;&gt;K is for K-fold Cross-Validation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There are multiple factors that go into choosing an appropriate model for your problem. These include things like performance, training speed, interpretability, and data types. That being said three of the most common model types are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Logistic regression&lt;/li&gt;
  &lt;li&gt;Random forests&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/blog/d-is-for-deep-learning&quot;&gt;Deep learning models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;building-a-cookie-classifier&quot;&gt;Building a cookie classifier&lt;/h3&gt;

&lt;h5 id=&quot;logistic-regression&quot;&gt;Logistic regression&lt;/h5&gt;

&lt;p&gt;Let’s imagine we have a training set where cookies are deemed acceptable or unacceptable. Here we have two features: the cookie area and the chocolate chip density. In the plot below the blue circles indicate acceptable cookies while the red squares are rejected (though I’d probably still eat them).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/s_is_for_supervised_learning/cookie_classification_logistic_regression.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Using this training set, we can train a model to predict which of some new set of cookies (the test set) will be acceptable. One way that we could use to do this is called &lt;strong&gt;logistic regression&lt;/strong&gt; which is a fairly popular algorithm. In logistic regression we are trying to learn the &lt;strong&gt;decision
boundary&lt;/strong&gt; which is shown above by the dotted black line. Points inside this circle are classified as acceptable while points outside are rejected. This decision boundary isn’t always a circle (in fact it usually isn’t). One nice feature of logistic regression is that you can have an arbitrarily shaped decision boundary (careful not to overfit!). As you can see, the classification isn't perfect and there are red squares misclassified as acceptable and vice versa. We can measure how well our classifier is doing using its &lt;a href=&quot;/blog/f-is-for-f1&quot;&gt;F1 score&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Classification algorithms typically have some level of confidence about their prediction (via a
probability). So it may have classified the blue circle on the edge of the decision boundary as unacceptable but if you looked at the confidence it would probably be pretty low. The same thing is true for the misclassified red squares. This is particularly useful if you are doing something like classifying executable code as malicious or benign. You can order your predictions in terms of confidence to have the most malicious at the top and then get less confident as you scroll downwards.&lt;/p&gt;

&lt;h5 id=&quot;random-forests&quot;&gt;Random forests&lt;/h5&gt;

&lt;p&gt;There are two key concepts involved in a random forest model that are more broadly used in data science/machine learning as a whole. The first is the idea of a &lt;strong&gt;decision tree&lt;/strong&gt;. A decision tree essentially looks like a flow chart. As you go down the tree, the data is categorized further
and further. The figure below shows an example decision tree of our cookie classification example. As you can see it is much closer to what a human would think if they were trying to determine whether the cookie should be sold or not.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/s_is_for_supervised_learning/decision_tree.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The second key idea is the notion of &lt;strong&gt;ensembles&lt;/strong&gt;. In an ensemble you take a bunch of weak learners (like a decision tree) and aggregate them together to create a strong learner. People also do ensembles with different machine learning algorithms (e.g. an ensemble of random
forest, logistic regression, and support vector machine models) where each model has a vote in the final classification. There are different ways to create an ensemble (e.g. votes can be weighted) and they are an extremely useful tool.&lt;/p&gt;

&lt;p&gt;The forest portion of random forest comes from the fact that it is an ensemble of decision trees. But where does the random come from? The random forest algorithm works as follows:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Sample data your training set (with replacement) to create random subsets of data&lt;/li&gt;
  &lt;li&gt;For each subset choose a random set of predictor features (e.g. cookie size)&lt;/li&gt;
  &lt;li&gt;For each node in the tree, find the predictor variable that provides the best split of the data. For example you might find that cookies under 5cm are not worth selling.  At the next node choose another set of features and repeat the process.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;As a typical example you might have 100 decision trees (all trained on random sets of features) in your random forest. Each tree makes a prediction if the cookie is worth selling/not. The final prediction is made based on the class that gets the most votes.&lt;/p&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;Supervised learning is an extremely powerful tool that can help us automate many tasks and scale to levels where humans cannot. A common use case for supervised learning models is to automate the simple decisions and have humans look at the more complex cases. As always it’s important to have &lt;a href=&quot;/blog/i-is-for-interpretability&quot;&gt;interpretable models&lt;/a&gt; as well as an &lt;a href=&quot;/blog/b-is-for-bias&quot;&gt;appeals process for wrong predictions&lt;/a&gt;. While model choices are important, data preprocessing and feature selection can have a large effect on model performance. It’s usually better to start with a simpler model like logistic regression or random forests before moving into more sophisticated models like deep learning.&lt;/p&gt;

&lt;h3 id=&quot;other-resources&quot;&gt;Other resources&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.coursera.org/learn/machine-learning&quot;&gt;Andrew Ng’s ML course&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://course.fast.ai/&quot;&gt;Practical machine learning for coders&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=4qVRBYAdLAo&quot;&gt;Crash Course AI #2: Supervised learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Supervised learning is one of the major categories of machine learning, and it helps us predict something about a given piece of data. Many of the concepts about supervised learning have come up in previous posts, but I’m hoping that this will provide a clearer picture of how they all fit together.</summary></entry><entry><title type="html">R is for Reproducibility</title><link href="https://abcsofdatascience.ca/blog/r-is-for-reproducibility" rel="alternate" type="text/html" title="R is for Reproducibility" /><published>2020-12-28T00:00:00-06:00</published><updated>2020-12-28T00:00:00-06:00</updated><id>https://abcsofdatascience.ca/blog/r-is-for-reproducibility</id><content type="html" xml:base="https://abcsofdatascience.ca/blog/r-is-for-reproducibility">&lt;p&gt;In data science (and all fields of science), being able to reproduce existing results is critical. One could argue being able to reproduce results is fundamentally what makes it science. However, many fields (including data science) are going through a “reproducibility crisis” where scientists are unable to recreate the results from their own or other experiments. There are many factors contributing to this such as&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Do you have the same materials (either physical or data) as the original experiment?&lt;/li&gt;
  &lt;li&gt;Can you recreate the experimental environment?&lt;/li&gt;
  &lt;li&gt;Are the methods listed detailed enough?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Everyone agrees that reproducibility is important but that doesn’t make it less of a challenge in practice. In this blog I’ll focus on reproducibility in data science but this also applies to other fields (particularly computational) of science as well.&lt;/p&gt;

&lt;h3 id=&quot;why-is-it-so-hard&quot;&gt;Why is it so hard?&lt;/h3&gt;

&lt;p&gt;It’s worth distinguishing between &lt;strong&gt;reproducibility&lt;/strong&gt; and &lt;strong&gt;repeatability&lt;/strong&gt;. Reproducibility is having another person (this can also be your future self) being able to fully recreate your results (either using the same or different methods). Repeatability (also known as replicability) is “given the same data and tools can you get the same result?”. In data science, people (including me!) tend to use the term reproducibility when they technically mean repeatability. For the rest of the blog I’ll use the two terms interchangeably.&lt;/p&gt;

&lt;p&gt;One of the reasons reproducibility is hard is that things change all the time. Data changes, the tools change, there could be randomness baked into an algorithm etc. There are also other systemic factors which I will touch on later. For now, let’s just focus on the technical aspects.&lt;/p&gt;

&lt;h3 id=&quot;do-you-have-the-same-data&quot;&gt;Do you have the same data?&lt;/h3&gt;

&lt;p&gt;Imagine you’re reading a paper about a model that can predict if a tweet is positive or negative. You want to reproduce their results so that you can apply the model to a different data set. You look at the methods section and read “the model was trained on a random sample of 100 000 tweets”. That’s great, but &lt;em&gt;which&lt;/em&gt; 100 000 tweets?&lt;/p&gt;

&lt;p&gt;There are many challenges associated having reproducible datasets&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The datasets need to be hosted somewhere (ideally somewhere you can interact with programatically). This could be something like an Amazon S3 bucket or &lt;a href=&quot;https://www.kaggle.com/datasets&quot;&gt;Kaggle&lt;/a&gt;. This can be expensive with large datasets.&lt;/li&gt;
  &lt;li&gt;The dataset should be static (or you should at least know how it’s changed). If you are reproducing a model trained on IMDB movie reviews but there are now 25000 more reviews in the dataset, this could affect the results.&lt;/li&gt;
  &lt;li&gt;It’s not enough to have the same raw data, the &lt;a href=&quot;(/blog/m-is-for-munging-data)&quot;&gt;processing pipeline should be the same&lt;/a&gt;. For example, how did the original authors handle missing data?&lt;/li&gt;
  &lt;li&gt;The dataset might be sensitive! For example you might have a model trained on healthcare data where it is hard to share the underlying data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Unfortunately there isn’t a solution to this problem that’s been widely adopted. There are a couple data version control solutions (e.g. https://dvc.org/) but many organizations create their own infrastructure (if they do anything at all). Many cloud providers provide dataset versioning but this can be expensive (especially for large datasets).&lt;/p&gt;

&lt;h3 id=&quot;do-you-have-the-same-computational-environment&quot;&gt;Do you have the same computational environment?&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/images/r_is_for_reproducibility/phd_comics_scratch.png&quot; alt=&quot;&quot; title=&quot;Taken from http://phdcomics.com/comics/archive.php?comicid=1689&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Fortunately, in data science it is easier to be able to replicate the experimental environment. In theory you are able to rerun the same code that other researchers used. This assumes a few things:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The other researchers made their code available using a version control system like &lt;a href=&quot;https://github.com/&quot;&gt;github&lt;/a&gt; or &lt;a href=&quot;https://about.gitlab.com/&quot;&gt;gitlab&lt;/a&gt;. It’s far from guaranteed that the associated code for a paper is made available for reasons such as intellectual property or competitive advantage. There is a great website which lists &lt;a href=&quot;https://paperswithcode.com/&quot;&gt;popular papers and their implementations&lt;/a&gt; so you know before you read the paper that you will be able to see the code.&lt;/li&gt;
  &lt;li&gt;You are using the same operating system as the original researchers. Let’s imagine you have a Windows machine and they were running Linux. This might not seem like a big difference but it can cause discrepancies.&lt;/li&gt;
  &lt;li&gt;You have the same version of all the &lt;a href=&quot;https://xkcd.com/1987/&quot;&gt;code dependencies&lt;/a&gt;. What if the original researchers were using Tensorflow 1.15 and you have version 2.x installed? This could cause issues if the code behaviour has changed in some way.&lt;/li&gt;
  &lt;li&gt;You have access to similar infrastructure. If the original model was trained on 1000 GPUs and you have a laptop, it will be impossible to reproduce the results.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One way to standardize parts of the computational environment is to use a containerization solution like Docker. This allows you to have the same code, dependencies and runtime environment. This does not solve the infrastructure issue but it provides a decent solution. Unfortunately, using docker is also complicated so might have a high barrier to entry for a lot of scientists. Spinning up the required resources using a cloud provider is also possible, but again this costs money.&lt;/p&gt;

&lt;h3 id=&quot;do-you-have-the-same-model-parameters&quot;&gt;Do you have the same model parameters?&lt;/h3&gt;

&lt;p&gt;If you are able to reproduce the compute environment and data processing, you still might need to retrain the model. One way around this is for researchers to share pretrained models but this isn’t always done. If you need to retrain the model from scratch you might not get exactly the same results. It depends on if the algorithms are &lt;strong&gt;deterministic&lt;/strong&gt; or &lt;strong&gt;stochastic&lt;/strong&gt;. Deterministic means that given the same inputs, parameters, and initial conditions you will get the same output. Stochastic processes have randomness inherent in them so you will get different output for the same input if you run the algorithm multiple times. There are many places where randomness can pop up&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/blog/g-is-for-gradient-descent&quot;&gt;Stochastic gradient descent&lt;/a&gt; is commonly used for optimization (e.g. in most &lt;a href=&quot;/blog/d-is-for-deep-learning&quot;&gt;deep learning models&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;In many &lt;a href=&quot;/blog/e-is-for-embeddings&quot;&gt;embedding and dimension reduction&lt;/a&gt; algorithms there is randomness in the output. Points that are close in the higher dimensional space will still be close to points in the lower dimensional space, but the position of the points themselves may change&lt;/li&gt;
  &lt;li&gt;Many models initialize their weights randomly&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It’s possible to seed random number generators so that you get the same results when generating random numbers. However, these may be buried deep in libraries or not configurable.&lt;/p&gt;

&lt;h3 id=&quot;other-barriers&quot;&gt;Other barriers&lt;/h3&gt;

&lt;p&gt;There are other obstacles which prevent or discourage people from making truly reproducible models (or reproducing other peoples models). One of the biggest barriers is that reproducing others’ work is expensive both in terms of time and money. It can also be very frustrating when something doesn’t work as expected
In academia researchers must publish papers which are novel in some way. It is very hard to publish a paper which says “we were able to reproduce this other work”. Researchers are incentivized to research new things rather than validating and exploring prior work. Similarly, in industry people are motivated to work on new products rather than replicating prior work.&lt;/p&gt;

&lt;p&gt;Even if you are motivated, there are the challenges described above. Did the previous researchers make their data, compute environment, and methods available? One way to incentivize researchers to do this is to make it a mandatory component of publishing a paper.&lt;/p&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;As you might have learned, reproducibility is very important but it’s also challenging to do in practice. Fortunately it’s not all bad news. There are tools which are making it easier to do reproducible science for those who are willing to put in the work.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://mybinder.org/&quot;&gt;Binder&lt;/a&gt; makes it really easy to run someone else’s &lt;a href=&quot;https://jupyter.org/&quot;&gt;Jupyter notebook&lt;/a&gt;. A Jupyter notebook is an interactive notebook which allows you to have code, documentation, and images (e.g. plots) all in the same document. They’re great for exploratory data analysis and documentation.&lt;/li&gt;
  &lt;li&gt;Many cloud providers such as Azure provide &lt;a href=&quot;https://azure.microsoft.com/en-ca/services/machine-learning/#capabilities&quot;&gt;machine learning capabilities&lt;/a&gt;. This makes it easier to keep track of models, parameters, and datasets.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/hackalog/cookiecutter-easydata&quot;&gt;Easydata&lt;/a&gt; is a python library and git template to make it easier to do reproducible data science.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;other-resources&quot;&gt;Other resources&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.podbean.com/ew/pb-wxkjp-ecfa7f&quot;&gt;Alan Turing Institute podcast on reproducible data science&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=x7gukmVdAxw&amp;amp;list=PLGVZCDnMOq0qaT_ji1YQ5O4bWTEarj9J8&amp;amp;index=30&amp;amp;t=0s&quot;&gt;Up your bus number tutorial&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/data-sciences-reproducibility-crisis-b87792d88513&quot;&gt;Data Science’s Reproducibility Crisis&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://lineardigressions.com/episodes/2017/9/3/data-lineage&quot;&gt;Linear digressions episode on data lineage&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">In data science (and all fields of science), being able to reproduce existing results is critical. One could argue being able to reproduce results is fundamentally what makes it science. However, many fields (including data science) are going through a “reproducibility crisis” where scientists are unable to recreate the results from their own or other experiments. There are many factors contributing to this such as</summary></entry><entry><title type="html">Q is for Q-learning</title><link href="https://abcsofdatascience.ca/blog/q-is-for-q-learning" rel="alternate" type="text/html" title="Q is for Q-learning" /><published>2020-12-27T00:00:00-06:00</published><updated>2020-12-27T00:00:00-06:00</updated><id>https://abcsofdatascience.ca/blog/q-is-for-q-learning</id><content type="html" xml:base="https://abcsofdatascience.ca/blog/q-is-for-q-learning">&lt;p&gt;How can we train a computer to play a video game? Here the goal could be something like “beat a level as quickly as possible” or “collect as many points as you can” or both of those objectives at the same time.  This is a very different task than the ones we’ve seen previously, such as trying to tell if a picture contains a dog, or how to group data together. To accomplish this we can use a framework of algorithms called &lt;strong&gt;reinforcement learning&lt;/strong&gt; (or RL). Reinforcement learning tries to train an &lt;strong&gt;agent&lt;/strong&gt; to learn a set of actions to achieve some goal. RL is used in a wide variety of disciplines such as&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Physics/chemistry. We might want to improve the yield of a particular chemical reaction by changing things like temperature or concentration. Doing this manually is time consuming and requires a lot of resources/expertise. We can train an RL model to learn experimental parameters which will help optimize the reaction.&lt;/li&gt;
  &lt;li&gt;Financial trading&lt;/li&gt;
  &lt;li&gt;Robotics&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In all of these disciplines you need to have a simulation of the environment you want to explore. In the case of chemistry, you need to be able to simulate the effect of increasing the temperature of a chemical reaction. Learning to play a video game is an ideal task for RL since the simulation already exists.&lt;/p&gt;

&lt;h3 id=&quot;playing-super-mario-bros&quot;&gt;Playing Super Mario Bros&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/images/q_is_for_q_learning/super-mario-bros.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let’s imagine we’re trying to train a computer to learn to play Super Mario Bros. Here we have the computer (the agent) controlling how Mario can move. It has a &lt;strong&gt;set of actions&lt;/strong&gt; it can perform:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Move Mario to the left&lt;/li&gt;
  &lt;li&gt;Move Mario to the right&lt;/li&gt;
  &lt;li&gt;Run&lt;/li&gt;
  &lt;li&gt;Jump&lt;/li&gt;
  &lt;li&gt;Pause the game&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The overall goal for RL algorithms is to figure out what is the best set of actions to take for a given &lt;strong&gt;state&lt;/strong&gt;. For example, at the beginning of the level it’s better to move to the right instead of the left (or jumping in place). To figure out what the “best” set of actions is, we also need a specific goal such as “collect as many points as possible”. We do this by specifying something known as a &lt;strong&gt;reward function&lt;/strong&gt;. In reinforcement learning the agent tries a bunch of different actions to try and maximize this reward function.&lt;/p&gt;

&lt;p&gt;In the case of collecting as many points as possible, the reward function would go up the more points Mario gets. However, if our reward function is just based on the number of points, the agent will spend all it’s time trying to collect every coin, powerup etc. We also want to incentivize the agent to complete the level quickly. We can add penalties to the reward function to incentivize this behaviour. For example, we might subtract some number for each second Mario takes to complete the level. Or since Mario only has a small number of lives, we might subtract a large number from the reward function if Mario dies.&lt;/p&gt;

&lt;p&gt;Here is the general strategy for reinforcement learning:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Observe the environment&lt;/li&gt;
  &lt;li&gt;Take some action(s) (e.g. moving right) based on some strategy&lt;/li&gt;
  &lt;li&gt;Receive a reward/penalty&lt;/li&gt;
  &lt;li&gt;Update the strategy based on the reward/penalty&lt;/li&gt;
  &lt;li&gt;Perform many, many iterations until you find an optimal strategy&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In RL there is a tradeoff between &lt;strong&gt;exploration&lt;/strong&gt; and &lt;strong&gt;exploitation&lt;/strong&gt;. Exploitation means doing the actions which you know will give you a reward (e.g. going for a particular coin block and getting 100 points). However, there may be actions that give an even bigger reward (e.g. grabbing the top of the flagpole gives 5000 points). You need to have a balance between exploitation (keep doing what gives you value) and exploration (trying new actions).&lt;/p&gt;

&lt;h3 id=&quot;types-of-reinforcement-learning&quot;&gt;Types of reinforcement learning&lt;/h3&gt;

&lt;p&gt;There are two main categories of RL:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Model based&lt;/strong&gt;: Here the model estimates or predict the optimal strategy (or &lt;strong&gt;policy&lt;/strong&gt;) based on the reward function itself. For example, in a given state the algorithm can say “if I jump I know the reward function will increase”&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Model free&lt;/strong&gt;: Here the algorithm estimates the optimal policy without direct knowledge of the reward function. They rely on samples from previous iterations (e.g. jumping under a coin block gives a reward) to learn the best strategy. Q-learning is an example of a  model free RL algorithm.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;q-learning&quot;&gt;Q-learning&lt;/h3&gt;

&lt;p&gt;The “Q” in in Q-learning stands for &lt;strong&gt;quality&lt;/strong&gt;. In this case quality means how useful a certain action is for gaining some future reward. Here, the agent maintains a &lt;strong&gt;Q-table&lt;/strong&gt; which is a table of states and possible actions. The Q-table is used to calculate the maximum expected future reward for performing a certain action in a given state. Let’s walkthrough the Q-learning algorithm at a very high level&lt;/p&gt;

&lt;h5 id=&quot;step-1-q-table-initialization&quot;&gt;Step 1: Q-table initialization&lt;/h5&gt;

&lt;p&gt;Initially the Q-table is all zeroes (since we don’t know what actions will give rewards). In our Mario example, the initial Q-table could look like&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;State/action&lt;/th&gt;
      &lt;th&gt;Move left&lt;/th&gt;
      &lt;th&gt;Move right&lt;/th&gt;
      &lt;th&gt;Jump&lt;/th&gt;
      &lt;th&gt;Hold run button&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;First screen&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Second screen&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;…&lt;/td&gt;
      &lt;td&gt;…&lt;/td&gt;
      &lt;td&gt;…&lt;/td&gt;
      &lt;td&gt;…&lt;/td&gt;
      &lt;td&gt;…&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;End of level&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h5 id=&quot;step-2-choose-an-action&quot;&gt;Step 2: Choose an action&lt;/h5&gt;

&lt;p&gt;There are two ways of choosing an action: randomly or based on the maximum value in the Q-table. Initially, all of the actions will be random (this is exploration) as we learn which actions lead to rewards. Eventually, most of the actions will be based on the Q-table.&lt;/p&gt;

&lt;h5 id=&quot;step-3-perform-the-action&quot;&gt;Step 3: Perform the action&lt;/h5&gt;

&lt;p&gt;Here the agent performs the action (e.g. jumping). It’s worth noting that the order of the actions matters a lot. For example, if Mario is trying to jump over a pit he must first run to the right then jump (not jump then run to the right).&lt;/p&gt;

&lt;h5 id=&quot;step-4-calculate-the-reward&quot;&gt;Step 4: Calculate the reward&lt;/h5&gt;

&lt;p&gt;Here we measure the reward based on the agents actions.&lt;/p&gt;

&lt;h5 id=&quot;step-5-update-the-q-table&quot;&gt;Step 5: Update the Q-table&lt;/h5&gt;

&lt;p&gt;Here we update the Q-table based on the agents actions and the reward. This helps the agent learn the expected future reward for a series of actions.&lt;/p&gt;

&lt;h5 id=&quot;step-6-repeat&quot;&gt;Step 6: Repeat&lt;/h5&gt;

&lt;p&gt;Steps 2-5 are repeated many, many times until training is stopped or some condition is met. At the end of training we should hopefully have a strategy which maximizes the reward function. After training the agent can choose actions for a given state based on the maximum value in the Q-table. In our Mario example the agent should learn that the best strategy at the beginning of the level is to run right then jump on the goomba (instead of just choosing actions at random).&lt;/p&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;Reinforcement learning is a very powerful area of machine learning, and still an active area of research. When using RL it’s critical to make sure the reward function matches the objective you are trying to achieve. If you would like to try training RL algorithms in practice, I recommend looking at the tutorials for the &lt;a href=&quot;https://gym.openai.com/&quot;&gt;OpenAI gym package&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;other-resources&quot;&gt;Other resources&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://lineardigressions.com/episodes/2016/8/20/reinforcement-learning-for-artificial-intelligence&quot;&gt;Linear digressions episode on RL&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/&quot;&gt;Q-learning example in OpenAI gym&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/a-beginners-guide-to-reinforcement-learning-with-a-mario-bros-example-fa0e0563aeb7&quot;&gt;Using RL to beat Mario&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">How can we train a computer to play a video game? Here the goal could be something like “beat a level as quickly as possible” or “collect as many points as you can” or both of those objectives at the same time. This is a very different task than the ones we’ve seen previously, such as trying to tell if a picture contains a dog, or how to group data together. To accomplish this we can use a framework of algorithms called reinforcement learning (or RL). Reinforcement learning tries to train an agent to learn a set of actions to achieve some goal. RL is used in a wide variety of disciplines such as</summary></entry></feed>